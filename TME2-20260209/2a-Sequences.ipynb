{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Processing with HMMs and CRFs\n",
    "\n",
    "**The goal of this practical is to study sequence models in NLP.**\n",
    "\n",
    "We will work on Part-Of-Speech (POS) and optionally on chunking (gathering different groups in sentences). The datasets are from [CONLL 2000](https://www.clips.uantwerpen.be/conll2000/chunking/): \n",
    "- **Small corpus:** chtrain/chtest to understand the tools and models \n",
    "- **Larger corpus:** train/test to collect reliable experimental results\n",
    "\n",
    "\n",
    "# 1) HMMS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading POS/Chunking data\n",
    "def load(filename):\n",
    "    listeDoc = list()\n",
    "    with open(filename, \"r\") as f:\n",
    "        doc = list()\n",
    "        for ligne in f:\n",
    "            if len(ligne) < 2: # fin de doc\n",
    "                listeDoc.append(doc)\n",
    "                doc = list()\n",
    "                continue\n",
    "            mots = ligne.replace(\"\\n\",\"\").split(\" \")\n",
    "            doc.append((mots[0],mots[1])) # Change mots[1] -> mots[2] for chuncking\n",
    "    return listeDoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== loding ============\n",
    "# small corpus => ideal for first tests\n",
    "filename = \"./datasets/conll2000/chtrain.txt\" \n",
    "filenameT = \"./datasets//conll2000/chtest.txt\" \n",
    "\n",
    "# Larger corpus => To valide perf.\n",
    "# filename = \"ressources/conll2000/train.txt\" \n",
    "# filenameT = \"ressources/conll2000/test.txt\" \n",
    "\n",
    "alldocs = load(filename)\n",
    "alldocsT = load(filenameT)\n",
    "\n",
    "print(len(alldocs),\" docs read\")\n",
    "print(len(alldocsT),\" docs (T) read\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(alldocs[0])\n",
    "print(alldocsT[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a baseline POS model (without sequence)\n",
    "\n",
    "We will build a simple dictionary ```word => PoS label``` without taking into account any sequence information. We will compare the sequence models to this baseline.\n",
    "\n",
    "The dataset is a list a tuples with ```(word, POS)```. **Build a simple dictionary mapping each word to its PoS tag in the train set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary building \n",
    "dico = dict()\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: on the test set, there are unknown words...**. We will use the following simple strategy: \n",
    "```\n",
    "# remplace\n",
    "dico[cle] # crashing with an unknown key \n",
    "# by \n",
    "dico.get(cle, DefaultValue)\n",
    "```\n",
    "From a linguistic point of view, we can choose the default value as the majority PoS class, producing a stronger baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate test performances\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check: 1433 good predictions in test over 1896\n",
    "\n",
    "(1527 with 'NN' as default PoS value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMMs\n",
    "\n",
    "Here is a code for training HMM parameters and running decoding using the Viterbi algorithm. You should apply it to our PoS task. **N.B.: you should undersand the ```eps``` parmaters**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allx: list of observation sequences \n",
    "# allq: list os state sequences \n",
    "# N: nb states\n",
    "# K: nb observations\n",
    "\n",
    "def learnHMM(allx, allq, N, K, initTo1=True):\n",
    "    if initTo1:\n",
    "        eps = 1e-1 # You can play with this regularization parameter \n",
    "        A = np.ones((N,N))*eps\n",
    "        B = np.ones((N,K))*eps\n",
    "        Pi = np.ones(N)*eps\n",
    "    else:\n",
    "        A = np.zeros((N,N))\n",
    "        B = np.zeros((N,K))\n",
    "        Pi = np.zeros(N)\n",
    "    for x,q in zip(allx,allq):\n",
    "        Pi[int(q[0])] += 1\n",
    "        for i in range(len(q)-1):\n",
    "            A[int(q[i]),int(q[i+1])] += 1\n",
    "            B[int(q[i]),int(x[i])] += 1\n",
    "        B[int(q[-1]),int(x[-1])] += 1 # last transition\n",
    "    A = A/np.maximum(A.sum(1).reshape(N,1),1) # normalisation\n",
    "    B = B/np.maximum(B.sum(1).reshape(N,1),1) # normalisation\n",
    "    Pi = Pi/Pi.sum()\n",
    "    return Pi , A, B\n",
    "\n",
    "def viterbi(x,Pi,A,B):\n",
    "    T = len(x)\n",
    "    N = len(Pi)\n",
    "    logA = np.log(A)\n",
    "    logB = np.log(B)\n",
    "    logdelta = np.zeros((N,T))\n",
    "    psi = np.zeros((N,T), dtype=int)\n",
    "    S = np.zeros(T)\n",
    "    logdelta[:,0] = np.log(Pi) + logB[:,int(x[0])]\n",
    "    #forward\n",
    "    for t in range(1,T):\n",
    "        logdelta[:,t] = (logdelta[:,t-1].reshape(N,1) + logA).max(0) + logB[:,int(x[t])]\n",
    "        psi[:,t] = (logdelta[:,t-1].reshape(N,1) + logA).argmax(0)\n",
    "    # backward\n",
    "    logp = logdelta[:,-1].max()\n",
    "    S[T-1] = logdelta[:,-1].argmax()\n",
    "    for i in range(2,T+1):\n",
    "        S[int(T-i)] = psi[int(S[int(T-i+1)]),int(T-i+1)]\n",
    "    return S, logp #, delta, psi\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data encoding\n",
    "\n",
    "We will map each word to an index for traing the HMM (see code below):\n",
    "```\n",
    " The cat is in the garden => 1 2 3 4 1 5\n",
    "```\n",
    "We have to understand the dictionary functionning to retrieve the words corresponding to indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alldocs etant issu du chargement des données\n",
    "# la mise en forme des données est fournie ici\n",
    "# afin de produire des analyses qualitative, vous devez malgré tout comprendre le fonctionnement des dictionnaires\n",
    "\n",
    "buf = [[m for m,pos in d ] for d in alldocs]\n",
    "mots = []\n",
    "[mots.extend(b) for b in buf]\n",
    "mots = np.unique(np.array(mots))\n",
    "nMots = len(mots)+1 # mot inconnu\n",
    "\n",
    "mots2ind = dict(zip(mots,range(len(mots))))\n",
    "mots2ind[\"UUUUUUUU\"] = len(mots)\n",
    "\n",
    "buf2 = [[pos for m,pos in d ] for d in alldocs]\n",
    "cles = []\n",
    "[cles.extend(b) for b in buf2]\n",
    "cles = np.unique(np.array(cles))\n",
    "cles2ind = dict(zip(cles,range(len(cles))))\n",
    "\n",
    "nCles = len(cles)\n",
    "\n",
    "print(nMots,nCles,\" in the dictionary\")\n",
    "\n",
    "# mise en forme des données\n",
    "allx  = [[mots2ind[m] for m,pos in d] for d in alldocs]\n",
    "allxT = [[mots2ind.setdefault(m,len(mots)) for m,pos in d] for d in alldocsT]\n",
    "\n",
    "allq  = [[cles2ind[pos] for m,pos in d] for d in alldocs]\n",
    "allqT = [[cles2ind.setdefault(pos,len(cles)) for m,pos in d] for d in alldocsT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First doc:\n",
    "print(allx[0])\n",
    "print(allq[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You turn: apply HMMs to those data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HMM training \n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HMM decoding and performances evaluation\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check : 1564 in test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative Analyis:\n",
    "\n",
    "- With imshow on the parameters (ou d'un argsort), show what are the probable transition between labels.\n",
    "- Visualize the confusion matrices to understand what is challenging in this task\n",
    "- Find out examples that are corrected by Viterbi decoding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Conditional Random Fields (CRF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CRF are disciminative models** representing the conditional distribution $P( \\mathbf{y} | \\mathbf{x} , \\mathbf{w})$:\n",
    "\n",
    "$$ P( \\mathbf{y} | \\mathbf{x} , \\mathbf{w})  = \\frac{e^{\\mathbf{w}^T  \\psi(\\mathbf{x},\\mathbf{y}) } }{\\sum\\limits_{y' \\in \\mathcal{y}}e^{\\mathbf{w}^T  \\psi(\\mathbf{x},\\mathbf{y}') } } $$ \n",
    "        \n",
    "**In 'linear-chain' CRFs**, the feature functions include **unary terms $u_k$** ($\\sim$ $\\mathbf{B}$ matrix in HMMs) and **pairwise terms $p_k$** ($\\sim$ $\\mathbf{A}$ matrix in HMMs):\n",
    "\n",
    "$$ \\mathbf{w}^T \\psi(\\mathbf{x},\\mathbf{y}) = \\sum\\limits_{t=1}^T \\sum_{k=1}^K F_k(y_{t-1}, y_t, \\mathbf{x})  =   \\sum\\limits_{t=1}^T \\sum_{k=1}^K \\left[ u_k(y_t, \\mathbf{x}) + p_k(y_{t-1}, y_t, \\mathbf{x}) \\right]$$\n",
    "\n",
    "[<img src=\"https://thome.isir.upmc.fr/classes/RITAL/crf-obs2.png\" width=\"800\" >](https://thome.isir.upmc.fr/classes/RITAL/crf-obs2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can directly use resources from nltk: \n",
    "- [CRFTagger](https://tedboy.github.io/nlps/generated/generated/nltk.CRFTagger.html)\n",
    "- [PerceptronTagger](https://www.nltk.org/_modules/nltk/tag/perceptron.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-crfsuite\n",
    "from nltk.tag.crf import CRFTagger\n",
    "tagger = CRFTagger()\n",
    "tagger.train(alldocs, 'out/crf.model') # training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluating the model, as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check: 1720 bonnes réponses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perceptron\n",
    "from nltk.tag.perceptron    import PerceptronTagger\n",
    "tagger = PerceptronTagger(load=False)\n",
    "tagger.train(alldocs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check: 1737 bonnes réponses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going further\n",
    "\n",
    "- We test the application for PoS, we can run similar experiments for chunking (see parsing indication, very simple to load data)\n",
    "- Run  experiement on the larger dataset. This dataset is still largely used in research. This work can thus be included in your resume :)\n",
    "- Work will be purshed with word embeddings (next practical), and for [NER](https://www.clips.uantwerpen.be/conll2003/ner/) with RNNs (X. Tannier)\n",
    "- [State-of-the-art resources](https://github.com/stanfordnlp/stanza/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
