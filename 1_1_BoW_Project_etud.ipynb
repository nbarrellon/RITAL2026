{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_VDguVQM4-63"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import codecs\n",
    "import re\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vF1zblob4-65"
   },
   "source": [
    "# Données reconnaissance du locuteur (Chirac/Mitterrand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<100:5:C> Le Congo, que naguère le <nom> qualifia de \"refuge pour la liberté\", de \"base de départ pour la libération\", de \"môle pour la Résistance\", comment ne pas être heureux de s'y retrouver ?<br>\n",
    "<100:6:C> Comment ne pas y voir un signe ?<br>\n",
    "<100:7:C> Brazzaville n'est pas une capitale ordinaire.<br>\n",
    "<100:8:C> Les voies de la libre disposition des peuples et de leur coopération furent explorées il y a un demi siècle, ici, à Brazzaville.<br>\n",
    "<100:9:C> C'est à Brazzaville, encore, que quinze années plus tard fut proclamée la Communauté.<br>\n",
    "<100:10:C> A Brazzaville, que fut scellée la première union régionale des pays africains francophones.<br>\n",
    "<100:11:C> A Brazzaville, que l'Afrique de demain se dessine.<br>\n",
    "<100:12:M> Je ne sais ni pourquoi ni comment on s'est opposé il y a quelques douze années - douze ou treize ans - à la création de l'Université technologique.<br>\n",
    "<100:13:M> C'est vrai qu'il y a très souvent dans notre pays un refus de regarder droit devant soi, comme un souhait d'en rester là, une certaine peur du changement, c'est vrai.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Y6Rar7OS4-65"
   },
   "outputs": [],
   "source": [
    "# Chargement des données:\n",
    "def load_pres(fname):\n",
    "    alltxts = []\n",
    "    alllabs = []\n",
    "    s=codecs.open(fname, 'r','utf-8') # pour régler le codage\n",
    "    while True:\n",
    "        txt = s.readline()\n",
    "        if(len(txt))<5:\n",
    "            break\n",
    "        #\n",
    "        lab = re.sub(r\"<[0-9]*:[0-9]*:(.)>.*\",\"\\\\1\",txt) #recupère le label (C ou M)\n",
    "        txt = re.sub(r\"<[0-9]*:[0-9]*:.>(.*)\",\"\\\\1\",txt) #recupère le texte\n",
    "        if lab.count('M') >0:\n",
    "            alllabs.append(-1)\n",
    "        else:\n",
    "            alllabs.append(1) #si c'est Mitterand qui parle on met -1 si c'est Chirac, on met 1\n",
    "        alltxts.append(txt)\n",
    "    return alltxts,alllabs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r\"<[0-9]*:[0-9]*:(.)>.*\",\"\\\\1\",\"<100:6:C> Comment ne pas y voir un signe ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Comment ne pas y voir un signe ?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r\"<[0-9]*:[0-9]*:.>(.*)\",\"\\\\1\",\"<100:6:C> Comment ne pas y voir un signe ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WWtWciXn4-66"
   },
   "outputs": [],
   "source": [
    "fname = \"./corpus.tache1.learn.utf8\"\n",
    "alltxts,alllabs = load_pres(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jL8lHNd64-66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57413 57413\n",
      " Quand je dis chers amis, il ne s'agit pas là d'une formule diplomatique, mais de l'expression de ce que je ressens.\n",
      "\n",
      "1\n",
      " Et ce sentiment n'est pas également partagé, monsieur le maire, mais enfin il existe de tous côtés.\n",
      "\n",
      "-1\n",
      " Je compte sur vous.\n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(alltxts),len(alllabs))\n",
    "print(alltxts[0])\n",
    "print(alllabs[0])\n",
    "print(alltxts[13])\n",
    "print(alllabs[13])\n",
    "print(alltxts[-1])\n",
    "print(alllabs[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XV-gD6mk4-66"
   },
   "source": [
    "# Données classification de sentiments (films)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "w02DpJ7r4-67"
   },
   "outputs": [],
   "source": [
    "def load_movies(path2data): # 1 classe par répertoire\n",
    "    alltxts = [] # init vide\n",
    "    labs = []\n",
    "    cpt = 0\n",
    "    for cl in os.listdir(path2data): # parcours des fichiers d'un répertoire\n",
    "        for f in os.listdir(path2data+cl):\n",
    "            txt = open(path2data+cl+'/'+f).read()\n",
    "            alltxts.append(txt)\n",
    "            labs.append(cpt)\n",
    "        cpt+=1 # chg répertoire = cht classe\n",
    "\n",
    "    return alltxts,labs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MOjqRZ9I4-67"
   },
   "outputs": [],
   "source": [
    "path = \"./movies1000/\"\n",
    "\n",
    "alltxts,alllabs = load_movies(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Mx7rgXeF4-67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 2000\n",
      "armageddon , in itself , symbolizes everything that is wrong in modern filmmaking . \n",
      "stories have been replaced with special effects ; character development gets overshadowed by bad dialogue ; plotting consists of a bunch of shit getting blown up . \n",
      "armageddon is as stupid , as loud and as shallow as any movie you'll see come out this summer , or maybe even any other summer . \n",
      "but i loved every freaking minute of it . \n",
      "believe me , i'm just as shocked as you are . \n",
      "hell , i don't even know why i went to see it in the first place . \n",
      "the previews were so annoying that i predicted this was going to be the worst film of the year , or at least in the running . \n",
      "i'm sorry , but \" somebody dial 911 ! ! ! \" \n",
      "isn't quite the tagging that's going to sell a movie . \n",
      "it isn't too wise either to market the film using the movie's stupidest lines ( \" beam me up scotty \" - yeah , that sure is great writing . . . ) . \n",
      "i mean , let's face it ; armageddon's previews rival the truman show's as being some of the worst of the year . \n",
      "neither of them even come close to doing their respective films justice . \n",
      "of course , you all know the story . \n",
      "when the earth is threatened with total annihilation via an asteroid the size of texas , nasa calls in the us's top oil drillers ( ! ) to go into space ( ! ) and implant a nuclear device eight-hundred and someodd feet into the asteroid ( ! ) . \n",
      "in the coarse of all this mayhem , we are introduced to some interesting - and not so interesting - characters . \n",
      "belonging to the former group is rockhound ( steve buscemi ) , a horny little womanizing genius who's always full of wisecracks , even when flying into space at a huge amount of g's . \n",
      "also , there's the always cool-as-hell billy bob thornton as dan truman , the bigwig at nasa who recruits all the drillers . \n",
      "he kind of reminded me of ed harris in apollo 13 , only without the intensity and great lines to deliver . \n",
      "then on the flip side of the coin is the tired , contrived character of harry stamper ( bruce willis , who does the whole movie employing with annoying accent i can't quite place ) , the leader of the pack as well as liv tyler and ben affleck as the token lovers you must have in any summer movie . \n",
      "basically , that's about it . \n",
      "as i said , this is hardly a film about plot . \n",
      "it's another summer blockbuster with plot points that are beyond unbelievable and dialogue and characters that are mostly completely wooden . \n",
      "case in point : nasa doesn't know that there is even an asteroid on it's way until eighteen days before impact - huh ? \n",
      "another example : at one point in the movie , two children are playing with toy space shuttles in front of a poster of kennedy . \n",
      "how pretentious is that ? ? ? ! ! ! \n",
      "want another one ? \n",
      "okay ; before the oil drillers blast off into space , one of them starts singing \" leaving on a jet plane \" , and soon , all the rest join in . \n",
      "did michael bay attend the school of sappy filmmaking before he made this picture ? \n",
      "but naturally , all this sappiness , melodrama and special effects accumulate to one bitchin' time at the movies . \n",
      "and don't get me wrong - despite all of the things i found wrong with armageddon , i still very much enjoyed it . \n",
      "so even if you don't win one of mcdonald's free tickets , it's still definitely worth checking out . \n",
      "\n",
      "0\n",
      "at first i was intrigued by the strange cast and odd creatures on the galaxy quest trailer , but that was before i saw the film . \n",
      "now my view has completely changed . \n",
      "it's time to embrace for impact , because this is a very bumpy ride . \n",
      "the story begins with the cast of galaxy quest including : jason nesmith ( tim allen ) , alexander dane ( alan rickman ) , gwen demarco ( sigourney weaver ) signing autographs at a convention . \n",
      "they meet fans who dress up in costumes , fans who worship the ground they walk on , and a group of aliens called thermians who believe that they are the ultimate saviors against the dreaded alien colony lead by sarris ( robin sachs ) . \n",
      "of course they are unaware of this until they actually begin performing their duties , and meet the ugly aliens themselves . \n",
      "thus begins the long adventure to help save the thermians . \n",
      "the movie plays like a really bad star trek episode , in fact it's worse . \n",
      "i don't even think trekkies will appreciate this weak spoof , because quite frankly it's just not funny . \n",
      "all the jokes are basically collected observations from the series . \n",
      "one such continuous joke involves a simple crew member who believes he will die in space , because no extra on the tv show ever lives , as proven in the star trek series . \n",
      "creative jokes like this may seem like a clever idea , but not when they are used to death . \n",
      "a person can only take so much . \n",
      "we do not need to be tortured , especially when you have to pay for it . \n",
      "it's pretty bad when even tim allen is pitiful . \n",
      "it's not like i expected an oscar worthy performance form him , but a few laughs would have been helpful . \n",
      "speaking of acting , 2 fine talents were wasted as well . \n",
      "sigourney weaver was here to show cleavage , well at least it worked . \n",
      "it's pretty bad when the only entertaining value of the film is cleavage . \n",
      "it just shows you how disgraceful the film really is . \n",
      "alan rickman however was not so lucky . \n",
      "after his last hit ( dogma ) he embarrasses himself by doing this sloppy mess . \n",
      "it's just a shame to see talented actors and actresses throw their ability away . \n",
      "when the film couldn't get any worse , thankfully some nice special effects pop up . \n",
      "like many big blockbusters ( armageddon , the haunting to name a few ) they rely heavily on effects to help boost the film's box office results . \n",
      "times it works , but unfortunately we have to shell out hard earned money and suffer through this junk . \n",
      "when will it stop ? \n",
      "i am getting tired of being suckered into seeing such trash . \n",
      "it may look fine and dandy , but we need to at least have a story . \n",
      "is that too much to ask ? \n",
      "obviously it is . \n",
      "when galaxy quest finally ends , it literally crash lands . \n",
      "aside from the impressive looking creatures from industrial light and magic , it's an embarrassment to the cast , and it's embarrassing to the science fiction genre . \n",
      "it's not the least bit fun , nor was it entertaining . \n",
      "the only place where this movie belongs is to infinity and beyond . \n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(alltxts),len(alllabs))\n",
    "print(alltxts[0])\n",
    "print(alllabs[0])\n",
    "print(alltxts[-1])\n",
    "print(alllabs[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VemyBYQ4-67"
   },
   "source": [
    "# A) Transformation paramétrique du texte (pre-traitements)\n",
    "\n",
    "Vous devez tester, par exemple, les cas suivants:\n",
    "- transformation en minuscule ou pas\n",
    "- suppression de la ponctuation\n",
    "- transformation des mots entièrement en majuscule en marqueurs spécifiques\n",
    "- suppression des chiffres ou pas\n",
    "- conservation d'une partie du texte seulement (seulement la première ligne = titre, seulement la dernière ligne = résumé, ...)\n",
    "- stemming\n",
    "- ...\n",
    "\n",
    "\n",
    "Vérifier systématiquement sur un exemple ou deux le bon fonctionnement des méthodes sur deux documents (au moins un de chaque classe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "armageddon   in itself   symbolizes everything that is wrong in modern filmmaking    stories have been replaced with special effects   character development gets overshadowed by bad dialogue   plotting consists of a bunch of shit getting blown up    armageddon is as stupid   as loud and as shallow as any movie you ll see come out this summer   or maybe even any other summer    but i loved every freaking minute of it    believe me   i m just as shocked as you are    hell   i don t even know why i went to see it in the first place    the previews were so annoying that i predicted this was going to be the worst film of the year   or at least in the running    i m sorry   but   somebody dial           isn t quite the tagging that s going to sell a movie    it isn t too wise either to market the film using the movie s stupidest lines     beam me up scotty     yeah   that sure is great writing            i mean   let s face it   armageddon s previews rival the truman show s as being some of the worst of the year    neither of them even come close to doing their respective films justice    of course   you all know the story    when the earth is threatened with total annihilation via an asteroid the size of texas   nasa calls in the us s top oil drillers       to go into space       and implant a nuclear device eight hundred and someodd feet into the asteroid          in the coarse of all this mayhem   we are introduced to some interesting   and not so interesting   characters    belonging to the former group is rockhound   steve buscemi     a horny little womanizing genius who s always full of wisecracks   even when flying into space at a huge amount of g s    also   there s the always cool as hell billy bob thornton as dan truman   the bigwig at nasa who recruits all the drillers    he kind of reminded me of ed harris in apollo    only without the intensity and great lines to deliver    then on the flip side of the coin is the tired   contrived character of harry stamper   bruce willis   who does the whole movie employing with annoying accent i can t quite place     the leader of the pack as well as liv tyler and ben affleck as the token lovers you must have in any summer movie    basically   that s about it    as i said   this is hardly a film about plot    it s another summer blockbuster with plot points that are beyond unbelievable and dialogue and characters that are mostly completely wooden    case in point   nasa doesn t know that there is even an asteroid on it s way until eighteen days before impact   huh    another example   at one point in the movie   two children are playing with toy space shuttles in front of a poster of kennedy    how pretentious is that              want another one    okay   before the oil drillers blast off into space   one of them starts singing   leaving on a jet plane     and soon   all the rest join in    did michael bay attend the school of sappy filmmaking before he made this picture    but naturally   all this sappiness   melodrama and special effects accumulate to one bitchin  time at the movies    and don t get me wrong   despite all of the things i found wrong with armageddon   i still very much enjoyed it    so even if you don t win one of mcdonald s free tickets   it s still definitely worth checking out    \n",
      "Taille du vocabulaire= 30000\n",
      "['aa' 'aaa' 'aaaaaaaaah' 'aaaaaaaahhhh' 'aaaaaah' 'aaaahhhs' 'aaliyah'\n",
      " 'aalyah' 'aardman' 'aaron' 'aback' 'abandon' 'abandoned' 'abandoning'\n",
      " 'abandonment' 'abandons' 'abating' 'abba' 'abbe' 'abberation' 'abberline'\n",
      " 'abbots' 'abbott' 'abbotts' 'abbreviated' 'abby' 'abc' 'abdomen'\n",
      " 'abducted' 'abductees' 'abduction' 'abductions' 'abdul' 'abe' 'abel'\n",
      " 'aberdeen' 'aberration' 'abetted' 'abetting' 'abeyance' 'abhorrence'\n",
      " 'abhorrent' 'abider' 'abides' 'abiding' 'abigail' 'abiility' 'abilities'\n",
      " 'ability' 'abject'] ['zidler' 'ziggy' 'zilch' 'zimbabwe' 'zimmely' 'zimmer' 'zimmerly'\n",
      " 'zimmerman' 'zinger' 'zingers' 'zinnemman' 'zinnia' 'zip' 'zipped'\n",
      " 'zippel' 'zipper' 'zippers' 'zippy' 'zips' 'ziyi' 'zodiac' 'zoe' 'zombie'\n",
      " 'zombies' 'zombified' 'zone' 'zones' 'zoo' 'zoolander' 'zoologist' 'zoom'\n",
      " 'zooming' 'zooms' 'zoot' 'zophres' 'zorg' 'zorro' 'zsigmond' 'zucker'\n",
      " 'zuehlke' 'zuko' 'zukovsky' 'zulu' 'zundel' 'zurg' 'zus' 'zweibel'\n",
      " 'zwick' 'zwigoff' 'zycie']\n",
      "----------------- Liste des mots du 1er document ------------------------\n",
      "  (0, 1086)\t4\n",
      "  (0, 13765)\t1\n",
      "  (0, 26788)\t1\n",
      "  (0, 9177)\t1\n",
      "  (0, 27217)\t9\n",
      "  (0, 29767)\t3\n",
      "  (0, 16416)\t1\n",
      "  (0, 10207)\t2\n",
      "  (0, 26161)\t1\n",
      "  (0, 12060)\t2\n",
      "  (0, 1850)\t1\n",
      "  (0, 22589)\t1\n",
      "  (0, 29593)\t6\n",
      "  (0, 25648)\t2\n",
      "  (0, 8214)\t2\n",
      "  (0, 3507)\t2\n",
      "  (0, 6616)\t1\n",
      "  (0, 11147)\t1\n",
      "  (0, 18570)\t1\n",
      "  (0, 2996)\t1\n",
      "  (0, 1551)\t1\n",
      "  (0, 6688)\t2\n",
      "  (0, 19992)\t1\n",
      "  (0, 4564)\t1\n",
      "  (0, 2894)\t1\n",
      "  :\t:\n",
      "  (0, 1339)\t1\n",
      "  (0, 24203)\t1\n",
      "  (0, 23952)\t1\n",
      "  (0, 15405)\t1\n",
      "  (0, 19671)\t1\n",
      "  (0, 17049)\t1\n",
      "  (0, 23950)\t1\n",
      "  (0, 16004)\t1\n",
      "  (0, 27445)\t1\n",
      "  (0, 16665)\t1\n",
      "  (0, 11145)\t1\n",
      "  (0, 6517)\t1\n",
      "  (0, 27286)\t1\n",
      "  (0, 10643)\t1\n",
      "  (0, 26094)\t2\n",
      "  (0, 28793)\t1\n",
      "  (0, 16694)\t1\n",
      "  (0, 8724)\t1\n",
      "  (0, 29524)\t1\n",
      "  (0, 15867)\t1\n",
      "  (0, 10723)\t1\n",
      "  (0, 27407)\t1\n",
      "  (0, 6139)\t1\n",
      "  (0, 29714)\t1\n",
      "  (0, 3582)\t1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def preprocessPerso(text): #suppression de la ponctuation\n",
    "    punc = string.punctuation\n",
    "    punc += '\\n\\r\\t'\n",
    "    #suppression de la ponctuation\n",
    "    text = text.translate(str.maketrans(punc, ' '*len(punc)))\n",
    "    #suppression des chiffres\n",
    "    text = re.sub(r\"[0-9]\",\"\",text)\n",
    "    return text\n",
    "\n",
    "#Verification du filtrage\n",
    "print(preprocessPerso(alltxts[0]))\n",
    "max_df = 5\n",
    "stop_words = [\"the\",\"a\",\"and\",\"is\",\"if\",\"i\",\"in\",\"on\",\"of\",\"ll\",\"do\",\"does\",\"any\",\"all\",\"at\",\"it\",\"as\",\"you\"]\n",
    "vectorizer = CountVectorizer(preprocessor=preprocessPerso,lowercase=True,\\\n",
    "                             strip_accents='unicode',max_features=30000,stop_words=stop_words)\n",
    "\n",
    "\n",
    "X = vectorizer.fit_transform(alltxts) #matrice avec id ligne, id mot dans la ligne et frequence\n",
    "vocabulaire = vectorizer.get_feature_names_out()\n",
    "print(\"Taille du vocabulaire=\",len(vocabulaire))\n",
    "print(vocabulaire[:50],vocabulaire[-50:]) #donne les mots du vocabulaire\n",
    "print(\"----------------- Liste des mots du 1er document ------------------------\")\n",
    "print(X[0])\n",
    "#print(\"nombre de mots dans le 1er document:\",X[0].size)\n",
    "#print(\"----------------- Liste des mots du 2eme document ------------------------\")\n",
    "#print(X[1])\n",
    "#print(\"nombre de mots dans le 2eme document:\",X[1].size)\n",
    "\n",
    "#print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about accent affleck also always amount an annihilation annoying another apollo are armageddon asteroid attend bad basically bay be beam been before being believe belonging ben beyond bigwig billy blast blockbuster blown bob bruce bunch buscemi but by calls can case character characters checking children close coarse coin come completely consists contrived cool course dan days definitely deliver despite development device dial dialogue did doesn doing don drillers earth ed effects eight eighteen either employing enjoyed even every everything example face feet film filmmaking films first flip flying former found freaking free front full genius get gets getting go going great group hardly harris harry have he hell horny how huge huh hundred impact implant intensity interesting into introduced isn itself jet join just justice kennedy kind know leader least leaving let lines little liv loud loved lovers made market maybe mayhem mcdonald me mean melodrama michael minute modern mostly movie movies much must nasa naturally neither not nuclear off oil okay one only or other out overshadowed pack picture place plane playing plot plotting point points poster predicted pretentious previews quite recruits reminded replaced respective rest rival rockhound running said sappiness sappy school scotty see sell shallow shit shocked show shuttles side singing size so some somebody soon sorry space special stamper starts steve still stories story stupid stupidest summer sure symbolizes tagging texas that their them then there things this thornton threatened tickets time tired to token too top total toy truman two tyler unbelievable until up us using very via want was way we well went were when who whole why willis win wise wisecracks with without womanizing wooden worst worth writing wrong yeah year "
     ]
    }
   ],
   "source": [
    "#Mots conservés suite à notre filtrage\n",
    "mots = X.toarray()[0]\n",
    "for i in range(len(mots)):\n",
    "    if mots[i]!=0:\n",
    "        print(vocabulaire[i],end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfzCDZCt4-68",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# B) Extraction du vocabulaire (BoW)\n",
    "\n",
    "- **Exploration préliminaire des jeux de données**\n",
    "    - Quelle est la taille d'origine du vocabulaire?\n",
    "    - Que reste-t-il si on ne garde que les 100 mots les plus fréquents? [word cloud]\n",
    "    - Quels sont les 100 mots dont la fréquence documentaire est la plus grande? [word cloud]\n",
    "    - Quels sont les 100 mots les plus discriminants au sens de odds ratio? [word cloud]\n",
    "    - Quelle est la distribution d'apparition des mots (Zipf)\n",
    "    - Quels sont les 100 bigrammes/trigrammes les plus fréquents?\n",
    "\n",
    "- **Variantes de BoW**\n",
    "    - TF-IDF\n",
    "    - Réduire la taille du vocabulaire (min_df, max_df, max_features)\n",
    "    - BoW binaire\n",
    "    - Bi-grams, tri-grams\n",
    "    - **Quelles performances attendre ? Quels sont les avantages et les inconvénients des ces variantes?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PmdHiLd4-68"
   },
   "source": [
    "# C) Modèles de Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelle est la taille d'origine du vocabulaire?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"./corpus.tache1.learn.utf8\"\n",
    "alltxts,alllabs = load_pres(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from unidecode import unidecode\n",
    "\n",
    "def preprocessPerso(text): \n",
    "    #suppression de la ponctuation\n",
    "    punc = string.punctuation\n",
    "    punc += '\\n\\r\\t'\n",
    "    text = text.translate(str.maketrans(punc, ' '*len(punc)))\n",
    "    #suppression des chiffres\n",
    "    text = re.sub(r\"[0-9]\",\"\",text)\n",
    "    #suppression des accents\n",
    "    text = unidecode(text)\n",
    "    #mise en minuscule\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du vocabulaire= 27055\n",
      "['aaron' 'ab' 'abaissaient' 'abaissant' 'abaisse' 'abaissement'\n",
      " 'abaissent' 'abaisser' 'abandon' 'abandonne' 'abandonnee' 'abandonnees'\n",
      " 'abandonnent' 'abandonner' 'abandonnerai' 'abandonnes' 'abandonnons'\n",
      " 'abat' 'abattage' 'abattait' 'abatte' 'abattent' 'abattre' 'abattu'\n",
      " 'abattues' 'abbaye' 'abbayes' 'abbes' 'abc' 'abces' 'abdication'\n",
      " 'abdiquer' 'abdou' 'abdul' 'abeba' 'aberrant' 'aberration' 'aberrations'\n",
      " 'abi' 'abidjan' 'abime' 'abimee' 'abimer' 'abimes' 'abitur' 'abject'\n",
      " 'abjection' 'abm' 'abnegation' 'abois'] ['xviie' 'xviieme' 'xviiie' 'xviiieme' 'xxe' 'xxeme' 'xxes' 'xxie'\n",
      " 'xxieme' 'xxiie' 'xxiieme' 'yacireta' 'yacyreta' 'yalta' 'yamoussoukro'\n",
      " 'yangzhou' 'yantala' 'yaounde' 'yaourt' 'yaourts' 'yemen' 'yen' 'yenenga'\n",
      " 'yersin' 'yeux' 'ylang' 'yonne' 'york' 'yorktown' 'yoruba' 'you'\n",
      " 'yougoslave' 'yougoslaves' 'yougoslavie' 'young' 'yvelines' 'yvon'\n",
      " 'zagreb' 'zambie' 'zelande' 'zenith' 'zeniths' 'zerbo' 'zero' 'zeste'\n",
      " 'zeus' 'zinder' 'zitouna' 'zocalo' 'zola' 'zone' 'zones' 'zouaves'\n",
      " 'zurich']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(preprocessor=preprocessPerso)\n",
    "\n",
    "X = vectorizer.fit_transform(alltxts) #matrice avec id ligne, id mot dans la ligne et frequence\n",
    "vocabulaire = vectorizer.get_feature_names_out()\n",
    "print(\"Taille du vocabulaire=\",len(vocabulaire))\n",
    "print(vocabulaire[:50],vocabulaire[27000:-1]) #donne les mots du vocabulaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que reste-t-il si on ne garde que les 100 mots les plus fréquents? [word cloud]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du vocabulaire= 100\n",
      "['ai' 'au' 'aujourd' 'aussi' 'autres' 'aux' 'avec' 'avenir' 'avez' 'avons'\n",
      " 'bien' 'ce' 'cela' 'ces' 'cette' 'ceux' 'chacun' 'comme' 'dans' 'date'\n",
      " 'de' 'depuis' 'des' 'deux' 'developpement' 'dire' 'doit' 'dont' 'du'\n",
      " 'elle' 'en' 'encore' 'ensemble' 'entre' 'est' 'et' 'etat' 'ete' 'etre'\n",
      " 'europe' 'faire' 'fait' 'faut' 'francais' 'france' 'hui' 'ici' 'il' 'ils'\n",
      " 'je' 'la' 'le' 'les' 'leur' 'leurs' 'mais' 'meme' 'monde' 'monsieur' 'ne'\n",
      " 'nom' 'nos' 'notre' 'nous' 'on' 'ont' 'ou' 'paix' 'par' 'pas' 'pays'\n",
      " 'peut' 'plus' 'politique' 'pour' 'president' 'qu' 'que' 'qui' 'sa' 'sans'\n",
      " 'se' 'ses' 'si' 'son' 'sont' 'sur' 'temps' 'tous' 'tout' 'toute' 'toutes'\n",
      " 'tres' 'un' 'une' 'union' 'vie' 'vos' 'votre' 'vous']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(preprocessor=preprocessPerso,max_features=100)\n",
    "\n",
    "X = vectorizer.fit_transform(alltxts)\n",
    "vocabulaire = vectorizer.get_feature_names_out()\n",
    "print(\"Taille du vocabulaire=\",len(vocabulaire))\n",
    "print(vocabulaire) #donne les mots du vocabulaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ne conserve que les 100 mots les plus fréquents qui ne sont pas discriminants.\n",
    "On peut donc les éliminer à l'aide de l'option stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['ai' 'au' 'aujourd' 'aussi' 'autres' 'aux' 'avec' 'avenir' 'avez' 'avons'\n",
    " 'bien' 'ce' 'cela' 'ces' 'cette' 'ceux' 'chacun' 'comme' 'dans' 'date'\n",
    " 'de' 'depuis' 'des' 'deux' 'developpement' 'dire' 'doit' 'dont' 'du'\n",
    " 'elle' 'en' 'encore' 'ensemble' 'entre' 'est' 'et' 'etat' 'ete' 'etre'\n",
    " 'europe' 'faire' 'fait' 'faut' 'francais' 'france' 'hui' 'ici' 'il' 'ils'\n",
    " 'je' 'la' 'le' 'les' 'leur' 'leurs' 'mais' 'meme' 'monde' 'monsieur' 'ne'\n",
    " 'nom' 'nos' 'notre' 'nous' 'on' 'ont' 'ou' 'paix' 'par' 'pas' 'pays'\n",
    " 'peut' 'plus' 'politique' 'pour' 'president' 'qu' 'que' 'qui' 'sa' 'sans'\n",
    " 'se' 'ses' 'si' 'son' 'sont' 'sur' 'temps' 'tous' 'tout' 'toute' 'toutes'\n",
    " 'tres' 'un' 'une' 'union' 'vie' 'vos' 'votre' 'vous']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du vocabulaire= 27055\n",
      "['aaron' 'ab' 'abaissaient' 'abaissant' 'abaisse' 'abaissement'\n",
      " 'abaissent' 'abaisser' 'abandon' 'abandonne' 'abandonnee' 'abandonnees'\n",
      " 'abandonnent' 'abandonner' 'abandonnerai' 'abandonnes' 'abandonnons'\n",
      " 'abat' 'abattage' 'abattait' 'abatte' 'abattent' 'abattre' 'abattu'\n",
      " 'abattues' 'abbaye' 'abbayes' 'abbes' 'abc' 'abces' 'abdication'\n",
      " 'abdiquer' 'abdou' 'abdul' 'abeba' 'aberrant' 'aberration' 'aberrations'\n",
      " 'abi' 'abidjan' 'abime' 'abimee' 'abimer' 'abimes' 'abitur' 'abject'\n",
      " 'abjection' 'abm' 'abnegation' 'abois'] ['xviie' 'xviieme' 'xviiie' 'xviiieme' 'xxe' 'xxeme' 'xxes' 'xxie'\n",
      " 'xxieme' 'xxiie' 'xxiieme' 'yacireta' 'yacyreta' 'yalta' 'yamoussoukro'\n",
      " 'yangzhou' 'yantala' 'yaounde' 'yaourt' 'yaourts' 'yemen' 'yen' 'yenenga'\n",
      " 'yersin' 'yeux' 'ylang' 'yonne' 'york' 'yorktown' 'yoruba' 'you'\n",
      " 'yougoslave' 'yougoslaves' 'yougoslavie' 'young' 'yvelines' 'yvon'\n",
      " 'zagreb' 'zambie' 'zelande' 'zenith' 'zeniths' 'zerbo' 'zero' 'zeste'\n",
      " 'zeus' 'zinder' 'zitouna' 'zocalo' 'zola' 'zone' 'zones' 'zouaves'\n",
      " 'zurich']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(preprocessor=preprocessPerso,stop_words=stopwords)\n",
    "\n",
    "X = vectorizer.fit_transform(alltxts) #matrice avec id ligne, id mot dans la ligne et frequence\n",
    "vocabulaire = vectorizer.get_feature_names_out()\n",
    "print(\"Taille du vocabulaire=\",len(vocabulaire))\n",
    "print(vocabulaire[:50],vocabulaire[27000:-1]) #donne les mots du vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/nilsbarrellon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du vocabulaire= 26943\n",
      "['aaron' 'ab' 'abaissaient' 'abaissant' 'abaisse' 'abaissement'\n",
      " 'abaissent' 'abaisser' 'abandon' 'abandonne' 'abandonnee' 'abandonnees'\n",
      " 'abandonnent' 'abandonner' 'abandonnerai' 'abandonnes' 'abandonnons'\n",
      " 'abat' 'abattage' 'abattait' 'abatte' 'abattent' 'abattre' 'abattu'\n",
      " 'abattues' 'abbaye' 'abbayes' 'abbes' 'abc' 'abces' 'abdication'\n",
      " 'abdiquer' 'abdou' 'abdul' 'abeba' 'aberrant' 'aberration' 'aberrations'\n",
      " 'abi' 'abidjan' 'abime' 'abimee' 'abimer' 'abimes' 'abitur' 'abject'\n",
      " 'abjection' 'abm' 'abnegation' 'abois'] ['twinauter' 'type' 'types' 'typhon' 'typhons' 'typique' 'typiquement'\n",
      " 'typographie' 'tyrannie' 'tyrannies' 'tyrannique' 'tyrans' 'tyrrhenienne'\n",
      " 'tzigane' 'ubiquite' 'udf' 'ue' 'ueo' 'ugag' 'uhlans' 'uicn' 'ukraine'\n",
      " 'ukrainien' 'ukrainienne' 'ukrainiens' 'ulterieurement' 'ultimatum'\n",
      " 'ultime' 'ultimes' 'ultra' 'ultramarine' 'ultramarines' 'ultramodernes'\n",
      " 'ultraperipherie' 'ultraperipherique' 'ultraperipheriques' 'ultras'\n",
      " 'ulysse' 'unaf' 'unanime' 'unanimement' 'unanimes' 'unanimisme'\n",
      " 'unanimite' 'unapl' 'undergo' 'unes' 'unesco' 'uni' 'unice' 'unicef'\n",
      " 'unicite' 'uniculturel' 'unie' 'unies' 'unificateur' 'unificateurs'\n",
      " 'unification' 'unificatrice' 'unifie' 'unifiee' 'unifient' 'unifier'\n",
      " 'uniforme' 'uniformement' 'uniformes' 'uniformisation' 'uniformisatrice'\n",
      " 'uniformise' 'uniformiser' 'uniformite' 'uniformites' 'unilateral'\n",
      " 'unilaterale' 'unilateralement' 'unilaterales' 'unilateralisme'\n",
      " 'unilateraliste' 'unilateralistes' 'unilinguistique' 'uninominal' 'union'\n",
      " 'unions' 'unipolaire' 'unique' 'uniquement' 'uniques' 'unir' 'unira'\n",
      " 'unirent' 'unis' 'unissaient' 'unissait' 'unissant' 'unisse' 'unissent'\n",
      " 'unissez' 'unissions' 'unisson' 'unissons' 'unit' 'unitaire' 'unite'\n",
      " 'unites' 'univers' 'universalise' 'universaliser' 'universalisme'\n",
      " 'universaliste' 'universalistes' 'universalite' 'universel' 'universelle'\n",
      " 'universellement' 'universelles' 'universels' 'universitaire'\n",
      " 'universitaires' 'universite' 'universites' 'unrwa' 'uns' 'unscom'\n",
      " 'untel' 'upa' 'uppsala' 'uranium' 'urbain' 'urbaine' 'urbaines' 'urbains'\n",
      " 'urban' 'urbanisation' 'urbanisme' 'urbanistes' 'uref' 'urgence'\n",
      " 'urgences' 'urgent' 'urgente' 'urgentes' 'urgents' 'uriops' 'urne'\n",
      " 'urnes' 'urologie' 'urss' 'urssaf' 'uruguay' 'uruguayen' 'uruguayenne'\n",
      " 'uruguayennes' 'uruguayens' 'urville' 'usage' 'usager' 'usagers' 'usages'\n",
      " 'usant' 'usent' 'user' 'uses' 'usez' 'ushuaia' 'usine' 'usines' 'usinor'\n",
      " 'ussel' 'usure' 'usurpe' 'utah' 'uterus' 'utile' 'utilement' 'utiles'\n",
      " 'utilisable' 'utilisables' 'utilisant' 'utilisateur' 'utilisateurs'\n",
      " 'utilisation' 'utilisations' 'utilisatrices' 'utilise' 'utilisee'\n",
      " 'utilisees' 'utilisent' 'utiliser' 'utilisera' 'utiliseront' 'utilises'\n",
      " 'utilisez' 'utilisions' 'utilisons' 'utilitariste' 'utilite' 'utopie'\n",
      " 'utopies' 'utopique' 'uvea' 'uvre' 'va' 'vacances' 'vacanciers' 'vacants'\n",
      " 'vaccin' 'vaccinales' 'vaccination' 'vaccins' 'vache' 'vaciller' 'vaclav'\n",
      " 'vague' 'vaguement' 'vagues' 'vaillance' 'vaillante' 'vaillantes'\n",
      " 'vaille' 'vaillent' 'vain' 'vaincra' 'vaincre' 'vaincrons' 'vaincu'\n",
      " 'vaincue' 'vaincues' 'vaincus' 'vaine' 'vainement' 'vaines' 'vainqueur'\n",
      " 'vainqueurs' 'vains' 'vais' 'vaisseau' 'vajpayee' 'vakalasi' 'val'\n",
      " 'valable' 'valables' 'valaient' 'valais' 'valait' 'valenciennes'\n",
      " 'valenciennois' 'valent' 'valery' 'valeur' 'valeureux' 'valeurs'\n",
      " 'validation' 'valide' 'valider' 'valides' 'validite' 'vallee' 'vallees'\n",
      " 'vallons' 'valmy' 'valoir' 'valois' 'valorisant' 'valorisation'\n",
      " 'valorise' 'valorisee' 'valorisent' 'valoriser' 'valoriseront'\n",
      " 'valorises' 'valse' 'valu' 'vandalisme' 'vanille' 'vanite' 'vanoise'\n",
      " 'vantais' 'vante' 'vanterai' 'vapeur' 'var' 'variable' 'variables'\n",
      " 'variante' 'variation' 'variations' 'varie' 'variee' 'variees' 'varient'\n",
      " 'varier' 'varies' 'variete' 'varsovie' 'vasarely' 'vasculaires' 'vase'\n",
      " 'vases' 'vassale' 'vaste' 'vastes' 'vatican' 'vatry' 'vaucluse' 'vaudra'\n",
      " 'vaudrait' 'vaudront' 'vaut' 've' 'veaux' 'vecten' 'vecteur' 'vecteurs'\n",
      " 'vecu' 'vecue' 'vecues' 'vecus' 'vecut' 'vedette' 'vedettes' 'vegetal'\n",
      " 'vegetales' 'vegetation' 'vehemence' 'vehemences' 'vehicule' 'vehiculee'\n",
      " 'vehicules' 'vehiculons' 'veilla' 'veillant' 'veille' 'veillent'\n",
      " 'veiller' 'veillera' 'veillerai' 'veillerais' 'veillerent' 'veillerons'\n",
      " 'veilleront' 'veilleur' 'veilleurs' 'veillez' 'veillons' 'veine' 'vel'\n",
      " 'velazquez' 'velizy' 'velleitaires' 'velleite' 'velleites' 'velodrome'\n",
      " 'velours' 'veme' 'venaient' 'venais' 'venaissin' 'venait' 'venant'\n",
      " 'venceslas' 'vend' 'vendee' 'vendent' 'vendeur' 'vendeurs' 'vendome'\n",
      " 'vendons' 'vendre' 'vendredi' 'vendriez' 'vendue' 'vendus' 'venerer'\n",
      " 'venetes' 'venez' 'venezuela' 'venge' 'vengeance' 'vengeances' 'venger'\n",
      " 'veniez' 'venins' 'venir' 'venise' 'venissieux' 'venons' 'vent' 'vente'\n",
      " 'ventes' 'ventre' 'vents' 'venu' 'venue' 'venues' 'venus' 'verbales'\n",
      " 'verbanha' 'verbania' 'verbanja' 'verbaux' 'verbe' 'vercingetorix'\n",
      " 'vercors' 'verdict' 'verdoyant' 'verdoyante' 'verdoyants' 'verdun'\n",
      " 'verdure' 'verifiable' 'verifiables' 'verificateurs' 'verification'\n",
      " 'verifications' 'verifie' 'verifiees' 'verifier' 'veritable'\n",
      " 'veritablement' 'veritables' 'verite' 'verites' 'vermeil' 'vernissage'\n",
      " 'verra' 'verrai' 'verraient' 'verrait' 'verre' 'verres' 'verrez'\n",
      " 'verrier' 'verrons' 'verront' 'verrous' 'vers' 'versailles' 'versant'\n",
      " 'verse' 'versee' 'versees' 'versement' 'versements' 'verses' 'version'\n",
      " 'vert' 'verte' 'vertes' 'vertical' 'vertige' 'vertigineuse'\n",
      " 'vertigineuses' 'vertigineux' 'verts' 'vertu' 'vertueux' 'vertus' 'verve'\n",
      " 'vervins' 'vesoul' 'veste' 'vestiges' 'veterance' 'veterans'\n",
      " 'veterinaires' 'vetilleuse' 'veto' 'vetustes' 'veuille' 'veuillent'\n",
      " 'veuilles' 'veuillez' 'veulent' 'veut' 'veux' 'vexations' 'via'\n",
      " 'viabilite' 'viable' 'viaduc' 'viande' 'viatique' 'vibrant' 'vibrante'\n",
      " 'vibre' 'vibrer' 'vibrerent' 'vibreront' 'vicaire' 'vice' 'vices' 'vichy'\n",
      " 'vicieux' 'vicissitudes' 'victime' 'victimes' 'victoire' 'victoires'\n",
      " 'victor' 'victorienne' 'victorieuse' 'victorieusement' 'victorieux'\n",
      " 'vide' 'videe' 'video' 'videocommunication' 'videodisque' 'videotex'\n",
      " 'vider' 'vides' 'vie' 'vieil' 'vieillards' 'vieille' 'vieilles'\n",
      " 'vieillesse' 'vieilli' 'vieillies' 'vieillir' 'vieillissantes'\n",
      " 'vieillissement' 'vieillit' 'viejo' 'viendra' 'viendrai' 'viendraient'\n",
      " 'viendrait' 'viendrez' 'viendriez' 'viendront' 'vienne' 'viennent'\n",
      " 'viens' 'vient' 'vierge' 'vierges' 'vies' 'vietnam' 'vietnamien'\n",
      " 'vietnamienne' 'vietnamiennes' 'vietnamiens' 'vieux' 'vif' 'vifs' 'vigie'\n",
      " 'vigies' 'vigilance' 'vigilant' 'vigilante' 'vigilants' 'vigipirate'\n",
      " 'vigneron' 'vignerons' 'vignes' 'vignoble' 'vigoureuse' 'vigoureusement'\n",
      " 'vigoureuses' 'vigoureux' 'vigueur' 'vih' 'viie' 'viii' 'viiie' 'viiies'\n",
      " 'villacoublay' 'village' 'villageoises' 'villages' 'villas' 'ville'\n",
      " 'villeneuve' 'villers' 'villes' 'villette' 'villon' 'vilnius' 'vin'\n",
      " 'vingt' 'vingtaine' 'vingtieme' 'vingts' 'vinrent' 'vinssent' 'vint'\n",
      " 'vintry' 'vinyle' 'viol' 'violation' 'violations' 'violemment' 'violence'\n",
      " 'violences' 'violent' 'violente' 'violentes' 'violents' 'violer' 'violes'\n",
      " 'violons' 'viols' 'virage' 'virales' 'virent' 'virginie' 'virgule'\n",
      " 'virologie' 'virtualites' 'virtuel' 'virtuelle' 'virtuellement'\n",
      " 'virtuelles' 'virtuels' 'virtuosite' 'virulence' 'virus' 'vis' 'visa'\n",
      " 'visage' 'visages' 'visaient' 'visait' 'visant' 'visas' 'viscerale'\n",
      " 'vise' 'visee' 'visees' 'visent' 'viser' 'viseront' 'vises' 'visez'\n",
      " 'visibilite' 'visible' 'visibles' 'vision' 'visionnaire' 'visionnaires'\n",
      " 'visions' 'visitai' 'visitais' 'visitant' 'visite' 'visitee' 'visitees'\n",
      " 'visiter' 'visiterai' 'visiterons' 'visites' 'visiteur' 'visiteurs'\n",
      " 'visitez' 'vit' 'vitae' 'vital' 'vitale' 'vitales' 'vitalite' 'vitaux'\n",
      " 'vite' 'vitesse' 'vitesses' 'viticulteur' 'viticulture' 'vitraux'\n",
      " 'vitrine' 'vitrines' 'viva' 'vivace' 'vivaces' 'vivacite' 'vivaient'\n",
      " 'vivais' 'vivait' 'vivant' 'vivante' 'vivantes' 'vivants' 'vive'\n",
      " 'vivement' 'vivent' 'vives' 'vivez' 'vivier' 'vivifiee' 'vivifiees'\n",
      " 'vivifier' 'vivions' 'vivons' 'vivra' 'vivrai' 'vivraient' 'vivre'\n",
      " 'vivres' 'vivrieres' 'vivrions' 'vivrons' 'vivront' 'vocable' 'vocables'\n",
      " 'vocabulaire' 'vocation' 'vocations' 'voeu' 'voeux' 'vogue' 'voi'\n",
      " 'voices' 'voici' 'voie' 'voient' 'voies' 'voila' 'voile' 'voilons' 'voir'\n",
      " 'voire' 'voirie' 'vois' 'voisin' 'voisinage' 'voisinages' 'voisine'\n",
      " 'voisines' 'voisins' 'voit' 'voiture' 'voitures' 'voivodie' 'voivodies'\n",
      " 'voix' 'vol' 'volaille' 'volailles' 'volant' 'volatilisation'\n",
      " 'volatilite' 'volcaniques' 'volcans' 'vole' 'volent' 'voler' 'voles'\n",
      " 'volet' 'volets' 'volk' 'volontaire' 'volontairement' 'volontaires'\n",
      " 'volontariat' 'volontariats' 'volontarisme' 'volontariste'\n",
      " 'volontaristes' 'volonte' 'volontes' 'volontiers' 'vols' 'volta'\n",
      " 'voltaire' 'volte' 'voltigeurs' 'volume' 'volumes' 'volumineux' 'vont'\n",
      " 'vote' 'votee' 'votees' 'votent' 'voter' 'votes' 'votive' 'votres'\n",
      " 'vouaient' 'voudra' 'voudrai' 'voudraient' 'voudrais' 'voudrait'\n",
      " 'voudrez' 'voudriez' 'voudrions' 'voudront' 'voue' 'vouee' 'vouees'\n",
      " 'vouent' 'voueront' 'voues' 'voulaient' 'voulais' 'voulait' 'voulant'\n",
      " 'voulez' 'vouliez' 'voulions' 'vouloir' 'vouloirs' 'voulons' 'voulu'\n",
      " 'voulue' 'voulues' 'voulus' 'voulut' 'voute' 'voyage' 'voyageant'\n",
      " 'voyagent' 'voyager' 'voyageront' 'voyages' 'voyageur' 'voyageurs'\n",
      " 'voyaient' 'voyais' 'voyait' 'voyant' 'voyants' 'voyez' 'voyions'\n",
      " 'voyons' 'voyous' 'vrai' 'vraie' 'vraiement' 'vraies' 'vraiment' 'vrais'\n",
      " 'vraisemblablement' 'vraisemblance' 'vu' 'vue' 'vues' 'vuh' 'vulgaire'\n",
      " 'vulgarisation' 'vulnerabilite' 'vulnerabilites' 'vulnerable'\n",
      " 'vulnerables' 'vus' 'wagon' 'wagons' 'wallis' 'wallisiens' 'ward'\n",
      " 'warsmann' 'washington' 'waterloo' 'waterpolo' 'wawel' 'week' 'weimar'\n",
      " 'weissmann' 'west' 'what' 'wilemotte' 'williamsburg' 'wilson' 'wir'\n",
      " 'wisigothe' 'withstand' 'wohler' 'woods' 'wresinski' 'wuillaume'\n",
      " 'wycombe' 'wye' 'xaintrailles' 'xeme' 'xenophobe' 'xenophobie' 'xi' 'xie'\n",
      " 'xies' 'xii' 'xiie' 'xiiie' 'xiveme' 'xixe' 'xixeme' 'xve' 'xvie'\n",
      " 'xvieme' 'xviie' 'xviieme' 'xviiie' 'xviiieme' 'xxe' 'xxeme' 'xxes'\n",
      " 'xxie' 'xxieme' 'xxiie' 'xxiieme' 'yacireta' 'yacyreta' 'yalta'\n",
      " 'yamoussoukro' 'yangzhou' 'yantala' 'yaounde' 'yaourt' 'yaourts' 'yemen'\n",
      " 'yen' 'yenenga' 'yersin' 'yeux' 'ylang' 'yonne' 'york' 'yorktown'\n",
      " 'yoruba' 'you' 'yougoslave' 'yougoslaves' 'yougoslavie' 'young'\n",
      " 'yvelines' 'yvon' 'zagreb' 'zambie' 'zelande' 'zenith' 'zeniths' 'zerbo'\n",
      " 'zero' 'zeste' 'zeus' 'zinder' 'zitouna' 'zocalo' 'zola' 'zone' 'zones'\n",
      " 'zouaves' 'zurich']\n"
     ]
    }
   ],
   "source": [
    "# French stop words: nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "final_stopwords_list = stopwords.words('french')\n",
    "#print(final_stopwords_list)\n",
    "vectorizer = CountVectorizer(preprocessor=preprocessPerso,stop_words=final_stopwords_list)\n",
    "#vectorizer = TfidfVectorizer(stop_words=final_stopwords_list)\n",
    "X = vectorizer.fit_transform(alltxts)\n",
    "vocabulaire = vectorizer.get_feature_names_out()\n",
    "print(\"Taille du vocabulaire=\",len(vocabulaire))\n",
    "print(vocabulaire[:50],vocabulaire[26000:-1]) #donne les mots du vocabulaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quels sont les 100 mots dont la fréquence documentaire est la plus grande?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du vocabulaire= 100\n",
      "['ai' 'au' 'aujourd' 'aussi' 'autres' 'aux' 'avec' 'avenir' 'avez' 'avons'\n",
      " 'bien' 'ce' 'cela' 'ces' 'cette' 'ceux' 'chacun' 'comme' 'dans' 'date'\n",
      " 'de' 'depuis' 'des' 'deux' 'developpement' 'dire' 'doit' 'dont' 'du'\n",
      " 'elle' 'en' 'encore' 'ensemble' 'entre' 'est' 'et' 'etat' 'ete' 'etre'\n",
      " 'europe' 'faire' 'fait' 'faut' 'francais' 'france' 'hui' 'ici' 'il' 'ils'\n",
      " 'je' 'la' 'le' 'les' 'leur' 'leurs' 'mais' 'meme' 'monde' 'monsieur' 'ne'\n",
      " 'nom' 'nos' 'notre' 'nous' 'on' 'ont' 'ou' 'paix' 'par' 'pas' 'pays'\n",
      " 'peut' 'plus' 'politique' 'pour' 'president' 'qu' 'que' 'qui' 'sa' 'sans'\n",
      " 'se' 'ses' 'si' 'son' 'sont' 'sur' 'temps' 'tous' 'tout' 'toute' 'toutes'\n",
      " 'tres' 'un' 'une' 'union' 'vie' 'vos' 'votre' 'vous']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "use_idf=True\n",
    "smooth_idf=True\n",
    "sublinear_tf=False\n",
    "\n",
    "vectorizer = TfidfVectorizer(preprocessor=preprocessPerso,use_idf= use_idf, smooth_idf=smooth_idf, sublinear_tf=sublinear_tf,max_features=100)\n",
    "X = vectorizer.fit_transform(alltxts)\n",
    "vocabulaire = vectorizer.get_feature_names_out()\n",
    "print(\"Taille du vocabulaire=\",len(vocabulaire))\n",
    "print(vocabulaire) #donne les mots du vocabulaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les 100 mots les plus fréquents se sont pas intéressant au niveau sémantique. On peut les supprimer en les ajoutant à la stop_word liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du vocabulaire= 26943\n",
      "['aaron' 'ab' 'abaissaient' 'abaissant' 'abaisse' 'abaissement'\n",
      " 'abaissent' 'abaisser' 'abandon' 'abandonne' 'abandonnee' 'abandonnees'\n",
      " 'abandonnent' 'abandonner' 'abandonnerai' 'abandonnes' 'abandonnons'\n",
      " 'abat' 'abattage' 'abattait' 'abatte' 'abattent' 'abattre' 'abattu'\n",
      " 'abattues' 'abbaye' 'abbayes' 'abbes' 'abc' 'abces' 'abdication'\n",
      " 'abdiquer' 'abdou' 'abdul' 'abeba' 'aberrant' 'aberration' 'aberrations'\n",
      " 'abi' 'abidjan' 'abime' 'abimee' 'abimer' 'abimes' 'abitur' 'abject'\n",
      " 'abjection' 'abm' 'abnegation' 'abois' 'abolie' 'abolir' 'abolirait'\n",
      " 'abolissent' 'abolit' 'abolition' 'abominable' 'abominables'\n",
      " 'abondamment' 'abondance' 'abondante' 'abondantes' 'abonde' 'abondent'\n",
      " 'abonder' 'abord' 'abordait' 'abordant' 'aborde' 'abordee' 'abordees'\n",
      " 'abordent' 'aborder' 'abordera' 'aborderai' 'aborderait' 'aborderons'\n",
      " 'aborderont' 'abordes' 'abordez' 'abordions' 'abordons' 'abords' 'abou'\n",
      " 'abouti' 'aboutir' 'aboutissait' 'aboutissant' 'aboutisse'\n",
      " 'aboutissement' 'aboutissements' 'aboutissent' 'aboutissions' 'aboutit'\n",
      " 'abraham' 'abreuver' 'abri' 'abritant' 'abrite' 'abritees'] ['vulnerables' 'vus' 'wagon' 'wagons' 'wallis' 'wallisiens' 'ward'\n",
      " 'warsmann' 'washington' 'waterloo' 'waterpolo' 'wawel' 'week' 'weimar'\n",
      " 'weissmann' 'west' 'what' 'wilemotte' 'williamsburg' 'wilson' 'wir'\n",
      " 'wisigothe' 'withstand' 'wohler' 'woods' 'wresinski' 'wuillaume'\n",
      " 'wycombe' 'wye' 'xaintrailles' 'xeme' 'xenophobe' 'xenophobie' 'xi' 'xie'\n",
      " 'xies' 'xii' 'xiie' 'xiiie' 'xiveme' 'xixe' 'xixeme' 'xve' 'xvie'\n",
      " 'xvieme' 'xviie' 'xviieme' 'xviiie' 'xviiieme' 'xxe' 'xxeme' 'xxes'\n",
      " 'xxie' 'xxieme' 'xxiie' 'xxiieme' 'yacireta' 'yacyreta' 'yalta'\n",
      " 'yamoussoukro' 'yangzhou' 'yantala' 'yaounde' 'yaourt' 'yaourts' 'yemen'\n",
      " 'yen' 'yenenga' 'yersin' 'yeux' 'ylang' 'yonne' 'york' 'yorktown'\n",
      " 'yoruba' 'you' 'yougoslave' 'yougoslaves' 'yougoslavie' 'young'\n",
      " 'yvelines' 'yvon' 'zagreb' 'zambie' 'zelande' 'zenith' 'zeniths' 'zerbo'\n",
      " 'zero' 'zeste' 'zeus' 'zinder' 'zitouna' 'zocalo' 'zola' 'zone' 'zones'\n",
      " 'zouaves' 'zurich' 'zx']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "use_idf=True\n",
    "smooth_idf=True\n",
    "sublinear_tf=False\n",
    "from nltk.corpus import stopwords\n",
    "final_stopwords_list = stopwords.words('french')\n",
    "stop_words = stopwords.words('french')+['ai' 'au' 'aujourd' 'aussi' 'autres' 'aux' 'avec' 'avenir' 'avez' 'avons'\n",
    " 'bien' 'ce' 'cela' 'ces' 'cette' 'ceux' 'chacun' 'comme' 'dans' 'date'\n",
    " 'de' 'depuis' 'des' 'deux' 'developpement' 'dire' 'doit' 'dont' 'du'\n",
    " 'elle' 'en' 'encore' 'ensemble' 'entre' 'est' 'et' 'etat' 'ete' 'etre'\n",
    " 'europe' 'faire' 'fait' 'faut' 'francais' 'france' 'hui' 'ici' 'il' 'ils'\n",
    " 'je' 'la' 'le' 'les' 'leur' 'leurs' 'mais' 'meme' 'monde' 'monsieur' 'ne'\n",
    " 'nom' 'nos' 'notre' 'nous' 'on' 'ont' 'ou' 'paix' 'par' 'pas' 'pays'\n",
    " 'peut' 'plus' 'politique' 'pour' 'president' 'qu' 'que' 'qui' 'sa' 'sans'\n",
    " 'se' 'ses' 'si' 'son' 'sont' 'sur' 'temps' 'tous' 'tout' 'toute' 'toutes'\n",
    " 'tres' 'un' 'une' 'union' 'vie' 'vos' 'votre' 'vous']\n",
    "vectorizer = TfidfVectorizer(preprocessor=preprocessPerso,use_idf= use_idf, smooth_idf=smooth_idf, \\\n",
    "                             sublinear_tf=sublinear_tf,stop_words=stop_words)\n",
    "X = vectorizer.fit_transform(alltxts)\n",
    "vocabulaire = vectorizer.get_feature_names_out()\n",
    "print(\"Taille du vocabulaire=\",len(vocabulaire))\n",
    "print(vocabulaire[:100],vocabulaire[-100:]) #donne les mots du vocabulaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quels sont les 100 mots les plus discriminants au sens de odds ratio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mondialisation    76.560328\n",
      "euro              65.929850\n",
      "partenariat       38.605431\n",
      "attentes          37.948305\n",
      "constituent       22.333734\n",
      "                    ...    \n",
      "compatriotes       8.217135\n",
      "chaleureux         8.167359\n",
      "moteur             8.022537\n",
      "ueo                8.022286\n",
      "ardeur             8.022286\n",
      "Length: 100, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Exemple de corpus (à remplacer par tes données)\n",
    "corpus1 = [alltxts[i] for i in range(len(alltxts)) if alllabs[i]==1]\n",
    "\n",
    "corpus2 = [alltxts[i] for i in range(len(alltxts)) if alllabs[i]==-1]\n",
    "\n",
    "# Nettoyage et vectorisation\n",
    "vectorizer = CountVectorizer(token_pattern=r'\\b[a-zA-Zàâäéèêëîïôöùûüçœ]{2,}\\b')\n",
    "X = vectorizer.fit_transform(corpus1 + corpus2)\n",
    "features = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Fréquences par corpus\n",
    "X1 = X[:len(corpus1)].sum(axis=0).A1\n",
    "X2 = X[len(corpus1):].sum(axis=0).A1\n",
    "\n",
    "# Calcul de l'odds ratio\n",
    "freq1 = pd.Series(X1, index=features)\n",
    "freq2 = pd.Series(X2, index=features)\n",
    "\n",
    "# Éviter les divisions par zéro\n",
    "freq1 = freq1 + 1\n",
    "freq2 = freq2 + 1\n",
    "\n",
    "odds_ratio = (freq1 / (freq1.sum() - freq1)) / (freq2 / (freq2.sum() - freq2))\n",
    "\n",
    "# Trier par odds ratio (descendant)\n",
    "top_100 = odds_ratio.sort_values(ascending=False).head(100)\n",
    "\n",
    "print(top_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C) Modèles de Machine Learning¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kP6dRY64-68"
   },
   "source": [
    "## 1) Métriques d'évaluation\n",
    "\n",
    "Il faudra utiliser des métriques d'évaluation pertinentes suivant la tâche et l'équilibrage des données :\n",
    "- Accuracy\n",
    "- Courbe ROC, AUC, F1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lA-XpjhI4-68"
   },
   "source": [
    "## 2) Variantes sur les stratégies d'entraînement\n",
    "\n",
    "- **Sur-apprentissage**. Les techniques sur lesquelles nous travaillons étant sujettes au sur-apprentissage: trouver le paramètre de régularisation dans la documentation et optimiser ce paramètre au sens de la métrique qui vous semble la plus appropriée (cf question précédente).\n",
    "\n",
    " <br>\n",
    "- **Equilibrage des données**. Un problème reconnu comme dur dans la communauté est celui de l'équilibrage des classes (*balance* en anglais). Que faire si les données sont à 80, 90 ou 99% dans une des classes?\n",
    "Le problème est dur mais fréquent; les solutions sont multiples mais on peut isoler 3 grandes familles de solution.\n",
    "\n",
    "1. Ré-équilibrer le jeu de données: supprimer des données dans la classe majoritaire et/ou sur-échantilloner la classe minoritaire.<BR>\n",
    "   $\\Rightarrow$ A vous de jouer pour cette technique\n",
    "1. Changer la formulation de la fonction de coût pour pénaliser plus les erreurs dans la classe minoritaire:\n",
    "soit une fonction $\\Delta$ mesurant les écarts entre $f(x_i)$ et $y_i$\n",
    "$$C = \\sum_i  \\alpha_i \\Delta(f(x_i),y_i), \\qquad \\alpha_i = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "1 & \\text{si } y_i \\in \\text{classe majoritaire}\\\\\n",
    "B>1 & \\text{si } y_i \\in \\text{classe minoritaire}\\\\\n",
    "\\end{array} \\right.$$\n",
    "<BR>\n",
    "   $\\Rightarrow$ Les SVM et d'autres approches sklearn possèdent des arguments pour régler $B$ ou $1/B$... Ces arguments sont utiles mais pas toujours suffisant.\n",
    "1. Courbe ROC et modification du biais. Une fois la fonction $\\hat y = f(x)$ apprise, il est possible de la *bidouiller* a posteriori: si toutes les prédictions $\\hat y$ sont dans une classe, on va introduire $b$ dans $\\hat y = f(x) + b$ et le faire varier jusqu'à ce qu'un des points change de classe. On peut ensuite aller de plus en plus loin.\n",
    "Le calcul de l'ensemble des scores associés à cette approche mène directement à la courbe ROC.\n",
    "\n",
    "**Note:** certains classifieurs sont intrinsèquement plus résistante au problème d'équilibrage, c'est par exemple le cas des techniques de gradient boosting que vous verrez l'an prochain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XShbhAtI4-68"
   },
   "source": [
    "## 3) Post-processing sur les données Président\n",
    "\n",
    "Pour la tâche de reconnaissance de locuteur, des phrases successives sont souvent associés à un même locuteur. Voir par exemples les 100 premiers labels de la base d'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O_5zd_T44-68"
   },
   "outputs": [],
   "source": [
    "fname = \"./datasets/AFDpresidentutf8/corpus.tache1.learn.utf8\"\n",
    "alltxts,alllabs = load_pres(fname)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(list(range(len(alllabs[0:100]))),alllabs[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TKW86g084-68"
   },
   "source": [
    "**Une méthode de post-traitement pour améliorer les résultats consistent à lisser les résultats de la prédictions d'une phrases par les prédictions voisines, en utilisant par exemple une convolution par une filtre Gaussien. Compléter la fonction ci-dessous et tester l'impact de ce lissage sur les performances.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A80IycAz4-68"
   },
   "outputs": [],
   "source": [
    "def gaussian_smoothing(pred, size):\n",
    "     # LISSAGE par un filtre Gaussien de taille size - vous pouvez utiliser np.convolve\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibBnd6A94-68"
   },
   "source": [
    "\n",
    "## 4) Estimer les performances de généralisation d'une méthodes\n",
    "**Ce sera l'enjeu principal du projet : vous disposez d'un ensemble de données, et vous évaluerez les performances sur un ensemble de test auquel vous n'avez pas accès. Il faut donc être capable d'estimer les performances de généralisation du modèles à partir des données d'entraînement.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYvtycCC4-68"
   },
   "source": [
    "\n",
    "Avant de lancer de grandes expériences, il faut se construire une base de travail solide en étudiant les questions suivantes:\n",
    "\n",
    "- Combien de temps ça prend d'apprendre un classifieur NB/SVM/RegLog sur ces données en fonction de la taille du vocabulaire?\n",
    "- La validation croisée est-elle nécessaire? Est ce qu'on obtient les mêmes résultats avec un simple *split*?\n",
    "- La validation croisée est-elle stable? A partir de combien de fold (travailler avec différentes graines aléatoires et faire des statistiques basiques)?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
