{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_XqRbsbUAzB"
   },
   "source": [
    "# NLP & representation learning: Neural Embeddings, Text Classification\n",
    "\n",
    "\n",
    "To use statistical classifiers with text, it is first necessary to vectorize the text. In the first practical session we explored the **Bag of Word (BoW)** model.\n",
    "\n",
    "Modern **state of the art** methods uses  embeddings to vectorize the text before classification in order to avoid feature engineering.\n",
    "\n",
    "## [Dataset](https://thome.isir.upmc.fr/classes/RITAL/json_pol.json)\n",
    "\n",
    "\n",
    "## \"Modern\" NLP pipeline\n",
    "\n",
    "By opposition to the **bag of word** model, in the modern NLP pipeline everything is **embeddings**. Instead of encoding a text as a **sparse vector** of length $D$ (size of feature dictionnary) the goal is to encode the text in a meaningful dense vector of a small size $|e| <<< |D|$.\n",
    "\n",
    "\n",
    "The raw classification pipeline is then the following:\n",
    "\n",
    "```\n",
    "raw text ---|embedding table|-->  vectors --|Neural Net|--> class\n",
    "```\n",
    "\n",
    "\n",
    "### Using a  language model:\n",
    "\n",
    "How to tokenize the text and extract a feature dictionnary is still a manual task. To directly have meaningful embeddings, it is common to use a pre-trained language model such as `word2vec` which we explore in this practical.\n",
    "\n",
    "In this setting, the pipeline becomes the following:\n",
    "```\n",
    "      \n",
    "raw text ---|(pre-trained) Language Model|--> vectors --|classifier (or fine-tuning)|--> class\n",
    "```\n",
    "\n",
    "\n",
    "- #### Classic word embeddings\n",
    "\n",
    " - [Word2Vec](https://arxiv.org/abs/1301.3781)\n",
    " - [Glove](https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "\n",
    "- #### bleeding edge language models techniques (see next)\n",
    "\n",
    " - [UMLFIT](https://arxiv.org/abs/1801.06146)\n",
    " - [ELMO](https://arxiv.org/abs/1802.05365)\n",
    " - [GPT](https://blog.openai.com/language-unsupervised/)\n",
    " - [BERT](https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Goal of this session:\n",
    "\n",
    "1. Train word embeddings on training dataset\n",
    "2. Tinker with the learnt embeddings and see learnt relations\n",
    "3. Tinker with pre-trained embeddings.\n",
    "4. Use those embeddings for classification\n",
    "5. Compare different embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHwH9L-RUAzE"
   },
   "source": [
    "## STEP 0: Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lLEIBkVgvLwQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "c-_T90jpUAzF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Although credit should have been given to Dr. Seuess for stealing the story-line of \"Horton Hatches The Egg\", this was a fine film. It touched both the emotions and the intellect. Due especially to the incredible performance of seven year old Justin Henry and a script that was sympathetic to each character (and each one\\'s predicament), the thought provoking elements linger long after the tear jerking ones are over. Overall, superior acting from a solid cast, excellent directing, and a very powerful script. The right touches of humor throughout help keep a \"heavy\" subject from becoming tedious or difficult to sit through. Lastly, this film stands the test of time and seems in no way dated, decades after it was released.', 1], [\"This was one of those films I would always come across (be it on TV or cheap DVD), but never struck me to give it a shot as I thought I wasn't missing out on much. It was on one night and I thought oh well\\x85 why not. A good decision too, as I would kick myself for taking so long to get around to it. For me it left me impressed, as it's up there with Burt Reynold's best features ('Deliverance', 'White Lightning' and 'Boogie Nights') and streams back to those 70s/80s gritty, hardboiled urban crime thrillers that weren't afraid to be forebodingly obscure and go out of their way to set-up characters, pack-it with realistically brutal force and effectively incorporate the local locations (Atlanta being the case here) to the fold with grounded photography. In certain shades it kind of reminded me of 'Dirty Harry', but that's loosely. However it's saucily honed blues score with its simmering kicks, funky shifts and unhinged sounds, very much had me thinking of Lalo Schifrin's pulsating score he orchestrated for 'Dirty Harry'. The music soundtrack on the other hand is hit or miss.<br /><br />Sgt. Tom Sharky was an Atlantic narcotic agent before a slip-out during a bust saw him demoted to vice work. Along with his new squad they come across a prostitution ring, which catches their interest due to fact it's owned by one hard-to-track and to convict crime lord. What they dig up involves a prominent government figure and a call-girl which can give them some important names, but they must get to her before she's made a target.<br /><br />Burt Reynold's acts, but also directs in an unyieldingly firm and muscular fashion which would suit his laconically hard-nosed performance and Gerald Di Pego's thematically hard-bitten and taut screenplay (that was adapted from William Diehl's novel). Well he does show some sort of heart/insightful thoughts amongst that armor within the scenes involving the fetchingly able British actress Rachel Ward, be it the stake-out scenes when he's watching her from another building (and slowly becoming infatuated by her) to when they finally come together, but these latter interactions mid-way through do slow up the momentum but give it noir like strokes. The performances are fairly spot on with Reynold's formulating a great rapport with exceptional actors Charles Durning, Earl Holliman, Brian Keith, Richard Libertini and Bernie Casey. The scathing profanity and witty dialogues between these guys were a blast. As for the corrupt villains, Vittorio Gassman builds imposing strength and power, but it's Henry Silva (who seems born for these roles) icily cunning and unstoppable turn that makes the show. Where his appearance seems to outline things to come and help them fall into place. Plus his adrenaline-filled and violent cat and mouse climax with Sharky and his team is brilliantly done.<br /><br />The exciting action passages might be quick and dry, but remain lethally violent like an immensely teeth-grinding interrogation sequence. Some handy, old fashion filming techniques add to the suspense. The intriguing material keeps it quite tactical being character derived, but when we think its smooth sailing it offers up a blunt surprise or two along with some intensely brunt confrontations.\", 1], [\"The Man Who Knew Too Much{1956}is a remake of a film that Alfred Hitchcock made in England in 1934 with the same name. In my opinion, his later effort is far superior. Many critics and fans of Alfred Hitchcock will argue that the remake is mediocre and doesn't have the spine tingling suspense of the original with Peter Lorre. In both films the plot is essentially the same, except the original is set in Switzerland and the remake in Marrakech . It tells the story of a married couple {James Stewart and Doris Day}vacationing with their young son and meeting a suspicious man, that is very curious about their past. It just so happens, he's an agent that's looking for a couple involved in a plot to assassinate a world leader.Then he gets stabbed in a Marrakeck market because of it being found out that he's a spy,and proceeds to fall into Stewart's arms.Dying,he tells him the whole story of the assassination plot.Stewart and Day then find out that another couple they met were the couple the agent was looking for and have kidnapped their son.The film contains excellent performances by Stewart and Day,in a straight dramatic role,as worried and frightened parents.This film proved that Doris Day could act in suspenseful dramas as well as carefree musicals.The direction by Alfred Hitchcock is top-notch.The film keeps you on the edge of your seat every minute.The scene in Albert Hall is a classic.The original is so slow-paced and drab.I don't know how people can compare the two.Just watch the remake and you'll enjoy it.I give the movie a 9 out of 10.\", 1]]\n",
      "Counter({1: 12500, 0: 12500})\n",
      "Number of reviews :  25000\n",
      "----> # of positive :  12500\n",
      "----> # of negative :  12500\n",
      "\n",
      "['Although credit should have been given to Dr. Seuess for stealing the story-line of \"Horton Hatches The Egg\", this was a fine film. It touched both the emotions and the intellect. Due especially to the incredible performance of seven year old Justin Henry and a script that was sympathetic to each character (and each one\\'s predicament), the thought provoking elements linger long after the tear jerking ones are over. Overall, superior acting from a solid cast, excellent directing, and a very powerful script. The right touches of humor throughout help keep a \"heavy\" subject from becoming tedious or difficult to sit through. Lastly, this film stands the test of time and seems in no way dated, decades after it was released.', 1]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Loading json\n",
    "file = './json_pol.json'\n",
    "\n",
    "with open(file,encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "#C'est une classe du module collections qui permet de compter les occurrences de chaque élément dans une collection \n",
    "#(liste, tuple, etc.). Elle retourne un dictionnaire où les clés sont les éléments uniques \n",
    "#de la collection, et les valeurs sont leurs comptes respectifs.\n",
    "\n",
    "# Quick Check\n",
    "print(data[:3])\n",
    "counter = Counter((x[1] for x in data))\n",
    "print(counter)\n",
    "print(\"Number of reviews : \", len(data))\n",
    "print(\"----> # of positive : \", counter[1])\n",
    "print(\"----> # of negative : \", counter[0])\n",
    "print(\"\")\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNpbHcJkUAzG"
   },
   "source": [
    "## Word2Vec: Quick Recap\n",
    "\n",
    "**[Word2Vec](https://arxiv.org/abs/1301.3781) is composed of two distinct language models (CBOW and SG), optimized to quickly learn word vectors**\n",
    "\n",
    "\n",
    "given a random text: `i'm taking the dog out for a walk`\n",
    "\n",
    "\n",
    "\n",
    "### (a) Continuous Bag of Word (CBOW)\n",
    "    -  predicts a word given a context\n",
    "    \n",
    "maximizing `p(dog | i'm taking the ___ out for a walk)`\n",
    "    \n",
    "### (b) Skip-Gram (SG)               \n",
    "    -  predicts a context given a word\n",
    "    \n",
    " maximizing `p(i'm taking the out for a walk | dog)`\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OD4odumEUAzG"
   },
   "source": [
    "## STEP 1: train a language model (word2vec)\n",
    "\n",
    "Gensim has one of [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html) fastest implementation.\n",
    "\n",
    "\n",
    "### Train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MceQ8uYrUAzG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gensim in /home/nilsbarrellon/.local/lib/python3.12/site-packages (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/lib/python3/dist-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /usr/lib/python3/dist-packages (from gensim) (1.11.4)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in /home/nilsbarrellon/.local/lib/python3.12/site-packages (from gensim) (7.5.0)\n",
      "Requirement already satisfied: wrapt in /home/nilsbarrellon/.local/lib/python3.12/site-packages (from smart_open>=1.8.1->gensim) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "# if gensim not installed yet\n",
    "! pip install gensim --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "cNb8inHjUAzG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-16 17:01:39,667 : INFO : collecting all words and their counts\n",
      "2026-02-16 17:01:39,667 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2026-02-16 17:01:40,052 : INFO : PROGRESS: at sentence #10000, processed 2301366 words, keeping 153853 word types\n",
      "2026-02-16 17:01:40,442 : INFO : PROGRESS: at sentence #20000, processed 4553558 words, keeping 240043 word types\n",
      "2026-02-16 17:01:40,638 : INFO : collected 276678 word types from a corpus of 5713167 raw words and 25000 sentences\n",
      "2026-02-16 17:01:40,638 : INFO : Creating a fresh vocabulary\n",
      "2026-02-16 17:01:40,791 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 48208 unique words (17.42% of original 276678, drops 228470)', 'datetime': '2026-02-16T17:01:40.791101', 'gensim': '4.4.0', 'python': '3.12.3 (main, Jan 22 2026, 20:57:42) [GCC 13.3.0]', 'platform': 'Linux-6.8.0-100-generic-x86_64-with-glibc2.39', 'event': 'prepare_vocab'}\n",
      "2026-02-16 17:01:40,791 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 5389596 word corpus (94.34% of original 5713167, drops 323571)', 'datetime': '2026-02-16T17:01:40.791722', 'gensim': '4.4.0', 'python': '3.12.3 (main, Jan 22 2026, 20:57:42) [GCC 13.3.0]', 'platform': 'Linux-6.8.0-100-generic-x86_64-with-glibc2.39', 'event': 'prepare_vocab'}\n",
      "2026-02-16 17:01:40,969 : INFO : deleting the raw counts dictionary of 276678 items\n",
      "2026-02-16 17:01:40,973 : INFO : sample=0.001 downsamples 44 most-common words\n",
      "2026-02-16 17:01:40,974 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 4165010.0598832574 word corpus (77.3%% of prior 5389596)', 'datetime': '2026-02-16T17:01:40.974609', 'gensim': '4.4.0', 'python': '3.12.3 (main, Jan 22 2026, 20:57:42) [GCC 13.3.0]', 'platform': 'Linux-6.8.0-100-generic-x86_64-with-glibc2.39', 'event': 'prepare_vocab'}\n",
      "2026-02-16 17:01:41,263 : INFO : estimated required memory for 48208 words and 100 dimensions: 62670400 bytes\n",
      "2026-02-16 17:01:41,263 : INFO : resetting layer weights\n",
      "2026-02-16 17:01:41,285 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2026-02-16T17:01:41.285124', 'gensim': '4.4.0', 'python': '3.12.3 (main, Jan 22 2026, 20:57:42) [GCC 13.3.0]', 'platform': 'Linux-6.8.0-100-generic-x86_64-with-glibc2.39', 'event': 'build_vocab'}\n",
      "2026-02-16 17:01:41,285 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 48208 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2026-02-16T17:01:41.285964', 'gensim': '4.4.0', 'python': '3.12.3 (main, Jan 22 2026, 20:57:42) [GCC 13.3.0]', 'platform': 'Linux-6.8.0-100-generic-x86_64-with-glibc2.39', 'event': 'train'}\n",
      "2026-02-16 17:01:42,311 : INFO : EPOCH 0 - PROGRESS: at 9.30% examples, 383359 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:01:43,318 : INFO : EPOCH 0 - PROGRESS: at 19.18% examples, 393325 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:01:44,330 : INFO : EPOCH 0 - PROGRESS: at 28.72% examples, 393885 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:01:45,373 : INFO : EPOCH 0 - PROGRESS: at 37.36% examples, 382409 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:01:46,381 : INFO : EPOCH 0 - PROGRESS: at 46.10% examples, 377050 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:01:47,388 : INFO : EPOCH 0 - PROGRESS: at 55.23% examples, 375521 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:01:48,402 : INFO : EPOCH 0 - PROGRESS: at 64.18% examples, 375590 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:01:49,415 : INFO : EPOCH 0 - PROGRESS: at 73.11% examples, 374080 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:01:50,435 : INFO : EPOCH 0 - PROGRESS: at 82.12% examples, 372562 words/s, in_qsize 6, out_qsize 0\n",
      "2026-02-16 17:01:51,448 : INFO : EPOCH 0 - PROGRESS: at 90.56% examples, 370171 words/s, in_qsize 6, out_qsize 0\n",
      "2026-02-16 17:01:52,466 : INFO : EPOCH 0 - PROGRESS: at 99.67% examples, 371181 words/s, in_qsize 3, out_qsize 0\n",
      "2026-02-16 17:01:52,495 : INFO : EPOCH 0: training on 5713167 raw words (4164785 effective words) took 11.2s, 371617 effective words/s\n",
      "2026-02-16 17:01:53,533 : INFO : EPOCH 1 - PROGRESS: at 8.38% examples, 337428 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:01:54,535 : INFO : EPOCH 1 - PROGRESS: at 17.84% examples, 363445 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:01:55,547 : INFO : EPOCH 1 - PROGRESS: at 26.75% examples, 364506 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:01:56,555 : INFO : EPOCH 1 - PROGRESS: at 35.24% examples, 361997 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:01:57,562 : INFO : EPOCH 1 - PROGRESS: at 43.43% examples, 356453 words/s, in_qsize 6, out_qsize 0\n",
      "2026-02-16 17:01:58,567 : INFO : EPOCH 1 - PROGRESS: at 51.56% examples, 352625 words/s, in_qsize 6, out_qsize 0\n",
      "2026-02-16 17:01:59,602 : INFO : EPOCH 1 - PROGRESS: at 59.71% examples, 348716 words/s, in_qsize 6, out_qsize 0\n",
      "2026-02-16 17:02:00,604 : INFO : EPOCH 1 - PROGRESS: at 67.23% examples, 345561 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:02:01,607 : INFO : EPOCH 1 - PROGRESS: at 75.35% examples, 343219 words/s, in_qsize 5, out_qsize 1\n",
      "2026-02-16 17:02:02,609 : INFO : EPOCH 1 - PROGRESS: at 83.18% examples, 341301 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:02:03,616 : INFO : EPOCH 1 - PROGRESS: at 90.89% examples, 339575 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:02:04,649 : INFO : EPOCH 1 - PROGRESS: at 99.67% examples, 341459 words/s, in_qsize 3, out_qsize 0\n",
      "2026-02-16 17:02:04,661 : INFO : EPOCH 1: training on 5713167 raw words (4165040 effective words) took 12.2s, 342399 effective words/s\n",
      "2026-02-16 17:02:05,679 : INFO : EPOCH 2 - PROGRESS: at 6.80% examples, 280339 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:02:06,690 : INFO : EPOCH 2 - PROGRESS: at 14.06% examples, 288103 words/s, in_qsize 6, out_qsize 0\n",
      "2026-02-16 17:02:07,694 : INFO : EPOCH 2 - PROGRESS: at 20.74% examples, 284323 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:02:08,703 : INFO : EPOCH 2 - PROGRESS: at 27.92% examples, 287583 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:02:09,731 : INFO : EPOCH 2 - PROGRESS: at 35.07% examples, 288511 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:02:10,744 : INFO : EPOCH 2 - PROGRESS: at 42.75% examples, 292126 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:02:11,754 : INFO : EPOCH 2 - PROGRESS: at 49.97% examples, 292756 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:02:12,786 : INFO : EPOCH 2 - PROGRESS: at 57.30% examples, 292614 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:02:13,822 : INFO : EPOCH 2 - PROGRESS: at 64.38% examples, 292488 words/s, in_qsize 6, out_qsize 0\n",
      "2026-02-16 17:02:14,836 : INFO : EPOCH 2 - PROGRESS: at 71.83% examples, 293841 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:02:15,856 : INFO : EPOCH 2 - PROGRESS: at 79.80% examples, 295440 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:02:16,877 : INFO : EPOCH 2 - PROGRESS: at 88.04% examples, 298992 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:02:17,905 : INFO : EPOCH 2 - PROGRESS: at 95.67% examples, 300214 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:02:18,626 : INFO : EPOCH 2: training on 5713167 raw words (4163472 effective words) took 14.0s, 298156 effective words/s\n",
      "2026-02-16 17:02:19,674 : INFO : EPOCH 3 - PROGRESS: at 7.87% examples, 314104 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:02:20,715 : INFO : EPOCH 3 - PROGRESS: at 17.03% examples, 338009 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:02:21,731 : INFO : EPOCH 3 - PROGRESS: at 23.94% examples, 321700 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:02:22,743 : INFO : EPOCH 3 - PROGRESS: at 30.84% examples, 311996 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:02:23,745 : INFO : EPOCH 3 - PROGRESS: at 37.56% examples, 306619 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:02:24,793 : INFO : EPOCH 3 - PROGRESS: at 44.62% examples, 301044 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:02:25,798 : INFO : EPOCH 3 - PROGRESS: at 51.94% examples, 300553 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:02:26,811 : INFO : EPOCH 3 - PROGRESS: at 59.38% examples, 301049 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:02:27,867 : INFO : EPOCH 3 - PROGRESS: at 66.90% examples, 301714 words/s, in_qsize 5, out_qsize 1\n",
      "2026-02-16 17:02:28,888 : INFO : EPOCH 3 - PROGRESS: at 74.94% examples, 303322 words/s, in_qsize 6, out_qsize 0\n",
      "2026-02-16 17:02:29,919 : INFO : EPOCH 3 - PROGRESS: at 82.83% examples, 304373 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:02:30,953 : INFO : EPOCH 3 - PROGRESS: at 90.56% examples, 305131 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:02:31,956 : INFO : EPOCH 3 - PROGRESS: at 97.98% examples, 305914 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:02:32,227 : INFO : EPOCH 3: training on 5713167 raw words (4164553 effective words) took 13.6s, 306235 effective words/s\n",
      "2026-02-16 17:02:33,248 : INFO : EPOCH 4 - PROGRESS: at 7.36% examples, 301465 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:02:34,264 : INFO : EPOCH 4 - PROGRESS: at 15.25% examples, 311597 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:02:35,273 : INFO : EPOCH 4 - PROGRESS: at 22.90% examples, 313631 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:02:36,280 : INFO : EPOCH 4 - PROGRESS: at 30.53% examples, 313256 words/s, in_qsize 5, out_qsize 1\n",
      "2026-02-16 17:02:37,305 : INFO : EPOCH 4 - PROGRESS: at 38.06% examples, 313249 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:02:38,318 : INFO : EPOCH 4 - PROGRESS: at 45.91% examples, 314037 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:02:39,374 : INFO : EPOCH 4 - PROGRESS: at 53.88% examples, 312455 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:02:40,394 : INFO : EPOCH 4 - PROGRESS: at 60.51% examples, 307719 words/s, in_qsize 6, out_qsize 0\n",
      "2026-02-16 17:02:41,417 : INFO : EPOCH 4 - PROGRESS: at 68.18% examples, 308825 words/s, in_qsize 6, out_qsize 0\n",
      "2026-02-16 17:02:42,437 : INFO : EPOCH 4 - PROGRESS: at 76.24% examples, 309752 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:02:43,446 : INFO : EPOCH 4 - PROGRESS: at 85.16% examples, 315266 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:02:44,481 : INFO : EPOCH 4 - PROGRESS: at 92.32% examples, 313249 words/s, in_qsize 5, out_qsize 0\n",
      "2026-02-16 17:02:45,508 : INFO : EPOCH 4 - PROGRESS: at 99.50% examples, 311839 words/s, in_qsize 4, out_qsize 0\n",
      "2026-02-16 17:02:45,577 : INFO : EPOCH 4: training on 5713167 raw words (4163544 effective words) took 13.3s, 311917 effective words/s\n",
      "2026-02-16 17:02:45,578 : INFO : Word2Vec lifecycle event {'msg': 'training on 28565835 raw words (20821394 effective words) took 64.3s, 323855 effective words/s', 'datetime': '2026-02-16T17:02:45.578723', 'gensim': '4.4.0', 'python': '3.12.3 (main, Jan 22 2026, 20:57:42) [GCC 13.3.0]', 'platform': 'Linux-6.8.0-100-generic-x86_64-with-glibc2.39', 'event': 'train'}\n",
      "2026-02-16 17:02:45,579 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=48208, vector_size=100, alpha=0.025>', 'datetime': '2026-02-16T17:02:45.579257', 'gensim': '4.4.0', 'python': '3.12.3 (main, Jan 22 2026, 20:57:42) [GCC 13.3.0]', 'platform': 'Linux-6.8.0-100-generic-x86_64-with-glibc2.39', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "text = [t.split() for t,p in data]\n",
    "\n",
    "# the following configuration is the default configuration\n",
    "w2v = gensim.models.word2vec.Word2Vec(sentences=text,\n",
    "                                vector_size=100, window=5,               ### here we train a cbow model\n",
    "                                min_count=5,\n",
    "                                sample=0.001, workers=3,\n",
    "                                sg=1, hs=0, negative=5,        ### set sg to 1 to train a sg model\n",
    "                                cbow_mean=1, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "UmGHjiCSUAzH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-16 17:03:28,618 : INFO : Word2Vec lifecycle event {'fname_or_handle': 'W2v-movies.dat', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2026-02-16T17:03:28.618267', 'gensim': '4.4.0', 'python': '3.12.3 (main, Jan 22 2026, 20:57:42) [GCC 13.3.0]', 'platform': 'Linux-6.8.0-100-generic-x86_64-with-glibc2.39', 'event': 'saving'}\n",
      "2026-02-16 17:03:28,619 : INFO : not storing attribute cum_table\n",
      "2026-02-16 17:03:28,694 : INFO : saved W2v-movies.dat\n"
     ]
    }
   ],
   "source": [
    "# Worth it to save the previous embedding\n",
    "w2v.save(\"W2v-movies.dat\")\n",
    "# You will be able to reload them:\n",
    "# w2v = gensim.models.Word2Vec.load(\"W2v-movies.dat\")\n",
    "# and you can continue the learning process if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.models.Word2Vec.load(\"W2v-movies.dat\") #pour ne pas relancer le processus un peu long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvZ_JcJWUAzH"
   },
   "source": [
    "## STEP 2: Test learnt embeddings\n",
    "\n",
    "The word embedding space directly encodes similarities between words: the vector coding for the word \"great\" will be closer to the vector coding for \"good\" than to the one coding for \"bad\". Generally, [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) is the distance used when considering distance between vectors.\n",
    "\n",
    "KeyedVectors have a built in [similarity](https://radimrehurek.com/gensim/models /keyedvectors.html#gensim.models.keyedvectors.BaseKeyedVectors.similarity) method to compute the cosine similarity between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "brrbED2dUAzH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great and good: 0.77287775\n",
      "great and bad: 0.45807964\n",
      "car and vehicle: 0.36336285\n",
      "boy and girl: 0.835626\n",
      "boy and man: 0.749683\n"
     ]
    }
   ],
   "source": [
    "# is great really closer to good than to bad ?\n",
    "print(\"great and good:\",w2v.wv.similarity(\"great\",\"good\"))\n",
    "print(\"great and bad:\",w2v.wv.similarity(\"great\",\"bad\"))\n",
    "print(\"car and vehicle:\",w2v.wv.similarity(\"car\",\"vehicle\"))\n",
    "print(\"boy and girl:\",w2v.wv.similarity(\"girl\",\"boy\"))\n",
    "print(\"boy and man:\",w2v.wv.similarity(\"man\",\"boy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IHLSxP19UAzI"
   },
   "source": [
    "Since cosine distance encodes similarity, neighboring words are supposed to be similar. The [most_similar](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.BaseKeyedVectors.most_similar) method returns the `topn` words given a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "iPKx8nJ1UAzI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', 0.9285252690315247),\n",
       " ('\"movie\"', 0.81612628698349),\n",
       " ('movie,', 0.7630794644355774),\n",
       " ('flick', 0.7629202008247375),\n",
       " ('\"film\"', 0.7519605159759521)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The query can be as simple as a word, such as \"movie\"\n",
    "\n",
    "# Try changing the word\n",
    "w2v.wv.most_similar(\"movie\",topn=5) # 5 most similar words\n",
    "#w2v.wv.most_similar(\"awesome\",topn=5)\n",
    "#w2v.wv.most_similar(\"actor\",topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrNM0r2PUAzI"
   },
   "source": [
    "But it can be a more complicated query\n",
    "Word embedding spaces tend to encode much more.\n",
    "\n",
    "The most famous exemple is: `vec(king) - vec(man) + vec(woman) => vec(queen)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "_vFcU_5bUAzI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('princess', 0.7496501803398132),\n",
       " ('witch', 0.7445906400680542),\n",
       " ('girl,', 0.7398815751075745),\n",
       " ('nun', 0.7359115481376648),\n",
       " ('prostitute', 0.7325125336647034),\n",
       " ('hooker', 0.7285485863685608),\n",
       " ('lady', 0.7254506945610046),\n",
       " ('girl\"', 0.719394326210022),\n",
       " ('baby,', 0.7187190055847168),\n",
       " ('Amudha', 0.7157213091850281)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is awesome - good + bad ?\n",
    "#w2v.wv.most_similar(positive=[\"awesome\",\"bad\"],negative=[\"good\"],topn=3)\n",
    "\n",
    "#w2v.wv.most_similar(positive=[\"actor\",\"woman\"],negative=[\"man\"],topn=3) # do the famous exemple works for actor ?\n",
    "\n",
    "w2v.wv.most_similar(positive=[\"king\",\"woman\",\"girl\"],negative=[\"man\"],topn=10)\n",
    "# Try other things like plurals for exemple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "915Y3erbUAzI"
   },
   "source": [
    "**To test learnt \"synctactic\" and \"semantic\" similarities, Mikolov et al. introduced a special dataset containing a wide variety of three way similarities.**\n",
    "\n",
    "**You can download the dataset [here](https://thome.isir.upmc.fr/classes/RITAL/questions-words.txt).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Nvw2_ZNnUAzI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-16 17:08:40,075 : INFO : Evaluating word analogies for top 300000 words in the model on questions-words.txt\n",
      "2026-02-16 17:08:40,288 : INFO : capital-common-countries: 2.2% (2/90)\n",
      "2026-02-16 17:08:40,441 : INFO : capital-world: 2.8% (2/71)\n",
      "2026-02-16 17:08:40,510 : INFO : currency: 0.0% (0/28)\n",
      "2026-02-16 17:08:41,111 : INFO : city-in-state: 0.0% (0/329)\n",
      "2026-02-16 17:08:41,696 : INFO : family: 32.5% (111/342)\n",
      "2026-02-16 17:08:43,246 : INFO : gram1-adjective-to-adverb: 2.4% (22/930)\n",
      "2026-02-16 17:08:44,201 : INFO : gram2-opposite: 3.8% (21/552)\n",
      "2026-02-16 17:08:46,643 : INFO : gram3-comparative: 20.7% (261/1260)\n",
      "2026-02-16 17:08:47,835 : INFO : gram4-superlative: 8.0% (56/702)\n",
      "2026-02-16 17:08:49,127 : INFO : gram5-present-participle: 19.7% (149/756)\n",
      "2026-02-16 17:08:50,699 : INFO : gram6-nationality-adjective: 3.3% (26/792)\n",
      "2026-02-16 17:08:52,910 : INFO : gram7-past-tense: 16.2% (204/1260)\n",
      "2026-02-16 17:08:54,428 : INFO : gram8-plural: 5.3% (43/812)\n",
      "2026-02-16 17:08:55,873 : INFO : gram9-plural-verbs: 25.1% (190/756)\n",
      "2026-02-16 17:08:55,875 : INFO : Quadruplets with out-of-vocabulary words: 55.6%\n",
      "2026-02-16 17:08:55,876 : INFO : NB: analogies containing OOV words were skipped from evaluation! To change this behavior, use \"dummy4unknown=True\"\n",
      "2026-02-16 17:08:55,879 : INFO : Total accuracy: 12.5% (1087/8680)\n"
     ]
    }
   ],
   "source": [
    "out = w2v.wv.evaluate_word_analogies(\"questions-words.txt\",case_insensitive=True)  #original semantic syntactic dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wS2kq8Q0UAzI"
   },
   "source": [
    "**When training the w2v models on the review dataset, since it hasn't been learnt with a lot of data, it does not perform very well.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3YP3tkF71hxH"
   },
   "source": [
    "## Word2vec visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Y7HaQi8A1giq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('France,', 0.8523359298706055),\n",
       " ('Venice,', 0.8267824053764343),\n",
       " ('London', 0.8234754800796509)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(positive=[\"France\",\"Paris\"],topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5rTAClPUAzI"
   },
   "source": [
    "## STEP 3: Loading a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9O-_X-ZoUAzI"
   },
   "source": [
    "In Gensim, embeddings are loaded and can be used via the [\"KeyedVectors\"](https://radimrehurek.com/gensim/models/keyedvectors.html) class\n",
    "\n",
    "> Since trained word vectors are independent from the way they were trained (Word2Vec, FastText, WordRank, VarEmbed etc), they can be represented by a standalone structure, as implemented in this module.\n",
    "\n",
    ">The structure is called “KeyedVectors” and is essentially a mapping between entities and vectors. Each entity is identified by its string id, so this is a mapping between {str => 1D numpy array}.\n",
    "\n",
    ">The entity typically corresponds to a word (so the mapping maps words to 1D vectors), but for some models, they key can also correspond to a document, a graph node etc. To generalize over different use-cases, this module calls the keys entities. Each entity is always represented by its string id, no matter whether the entity is a word, a document or a graph node.\n",
    "\n",
    "**You can download the pre-trained word embedding [HERE](https://thome.isir.upmc.fr/classes/RITAL/word2vec-google-news-300.dat) .**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "4QvDpH5_UAzJ",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-16 17:20:40,276 : INFO : Creating /home/nilsbarrellon/gensim-data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-16 17:32:46,301 : INFO : word2vec-google-news-300 downloaded\n",
      "2026-02-16 17:32:46,305 : INFO : loading projection weights from /home/nilsbarrellon/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n",
      "2026-02-16 17:33:18,236 : INFO : KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from /home/nilsbarrellon/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz', 'binary': True, 'encoding': 'utf8', 'datetime': '2026-02-16T17:33:18.236641', 'gensim': '4.4.0', 'python': '3.12.3 (main, Jan 22 2026, 20:57:42) [GCC 13.3.0]', 'platform': 'Linux-6.8.0-100-generic-x86_64-with-glibc2.39', 'event': 'load_word2vec_format'}\n",
      "2026-02-16 17:33:18,237 : INFO : KeyedVectors lifecycle event {'fname_or_handle': 'word2vec-google-news-300.dat', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2026-02-16T17:33:18.237482', 'gensim': '4.4.0', 'python': '3.12.3 (main, Jan 22 2026, 20:57:42) [GCC 13.3.0]', 'platform': 'Linux-6.8.0-100-generic-x86_64-with-glibc2.39', 'event': 'saving'}\n",
      "2026-02-16 17:33:18,238 : INFO : storing np array 'vectors' to word2vec-google-news-300.dat.vectors.npy\n",
      "2026-02-16 17:33:25,668 : INFO : saved word2vec-google-news-300.dat\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "bload = False\n",
    "fname = \"word2vec-google-news-300\"\n",
    "sdir = \"\" # Change\n",
    "\n",
    "if(bload==True):\n",
    "    wv_pre_trained = KeyedVectors.load(sdir+fname+\".dat\")\n",
    "else:\n",
    "    wv_pre_trained = api.load(fname)\n",
    "    wv_pre_trained.save(sdir+fname+\".dat\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "anDUhQm5UAzJ"
   },
   "source": [
    "**Perform the \"synctactic\" and \"semantic\" evaluations again. Conclude on the pre-trained embeddings.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEHiFK1bUAzJ"
   },
   "source": [
    "## STEP 4:  sentiment classification\n",
    "\n",
    "In the previous practical session, we used a bag of word approach to transform text into vectors.\n",
    "Here, we propose to try to use word vectors (previously learnt or loaded).\n",
    "\n",
    "\n",
    "### <font color='green'> Since we have only word vectors and that sentences are made of multiple words, we need to aggregate them. </font>\n",
    "\n",
    "\n",
    "### (1) Vectorize reviews using word vectors:\n",
    "\n",
    "Word aggregation can be done in different ways:\n",
    "\n",
    "- Sum\n",
    "- Average\n",
    "- Min/feature\n",
    "- Max/feature\n",
    "\n",
    "#### a few pointers:\n",
    "\n",
    "- `w2v.wv.vocab` is a `set()` of the vocabulary (all existing words in your model)\n",
    "- `np.minimum(a,b) and np.maximum(a,b)` respectively return element-wise min/max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "h8KCYX5iUAzJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  -3.6030931   81.09755     -5.5625744  -32.537807   -13.523959\n",
      " -149.08589     46.395557   178.24171    -49.124313   -56.371647\n",
      "   -6.567054  -116.84839    -30.888432    59.093906    37.168816\n",
      "  -65.31096     54.803596   -30.867128   -46.19165   -209.28311\n",
      "   30.657885   -39.241558   146.07489    -56.333878    -1.6967841\n",
      "   65.81456    -60.338875    -2.684317  -114.31843     37.438686\n",
      "   85.73729      4.8225527   -4.6872067 -117.13731    -15.731178\n",
      "   21.141417    -0.8496002  -82.54633    -68.87561   -157.08452\n",
      "    2.7410727 -131.53127    -72.49354     48.381435    60.973804\n",
      "  -30.774494   -56.276104    -8.592021    59.7097      24.78802\n",
      "   55.970375   -94.613655    37.70264     12.96038    -76.094986\n",
      "    7.6361637   42.042484     4.9743958  -74.76654     28.26746\n",
      "   28.929958  -110.046394    48.576397    61.086758  -115.15952\n",
      "  113.765495     2.3422656  129.27287   -124.883194    77.206985\n",
      "  -41.333717    91.983795    89.56144      5.6670938   89.5685\n",
      "   45.964237    31.110245     8.384906   -51.72349     -3.6369636\n",
      "  -67.07156    -11.790692   -96.72163     46.894432   -62.115597\n",
      "   -9.543389    83.29901     50.70346     93.65554      7.7531376\n",
      "  159.20456     96.08529     17.397665    16.127165   133.48457\n",
      "  -13.95604     90.20327    -80.80995     34.023586   -24.113264 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# We first need to vectorize text:\n",
    "# First we propose to a sum of them\n",
    "#Bibliothèques utiles\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def vectorize(text, w2v, mean=False):\n",
    "    \"\"\"\n",
    "    Vectorise un texte en utilisant un modèle Word2Vec.\n",
    "    :param text: Texte à vectoriser (str).\n",
    "    :param w2v: Modèle Word2Vec entraîné (gensim.models.Word2Vec).\n",
    "    :param mean: Si True, retourne la moyenne des vecteurs des mots. Sinon, retourne la somme.\n",
    "    :return: Vecteur NumPy (np.array) représentant le texte.\n",
    "    \"\"\"\n",
    "    vectors = []\n",
    "    for word in text.split():\n",
    "        if word in w2v.wv.key_to_index:  # Vérifie si le mot est dans le vocabulaire\n",
    "            vectors.append(w2v.wv[word])  # Ajoute le vecteur du mot\n",
    "\n",
    "    if not vectors:  # Si aucun mot n'est dans le vocabulaire\n",
    "        return np.zeros(w2v.vector_size)  # Retourne un vecteur nul\n",
    "\n",
    "    if mean:\n",
    "        return np.mean(vectors, axis=0)  # Moyenne des vecteurs\n",
    "    else:\n",
    "        return np.sum(vectors, axis=0)  # Somme des vecteurs\n",
    "#on coupe le jeu de données en 80% de données d'entrainement, 20% de données pour les tests\n",
    "\n",
    "[train, test]  = train_test_split(data, test_size=0.2, random_state=42, shuffle=True)\n",
    "#label d'entrainement\n",
    "classes = [pol for text,pol in train]\n",
    "#données d'entrainement\n",
    "X = [vectorize(text,w2v) for text,pol in train]\n",
    "#données de test\n",
    "X_test = [vectorize(text,w2v) for text,pol in test]\n",
    "#labels de test\n",
    "true = [pol for text,pol in test]\n",
    "\n",
    "#let's see what a review vector looks like.\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NwhBgVzdUAzJ"
   },
   "source": [
    "### (2) Train a classifier\n",
    "as in the previous practical session, train a logistic regression to do sentiment classification with word vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "CpZKbMsaUAzJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Logistic Regression accuracy train=0.8297, test=0.8290\n",
      "Logistic Regression recall train=0.8297, test=0.8290\n",
      "Logistic Regression F1-score train=0.8297, test=0.8290\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,recall_score, f1_score\n",
    "\n",
    "\n",
    "#Logistic Regression\n",
    "t = 1e-8\n",
    "C=100.0\n",
    "lr_clf = LogisticRegression(random_state=0, solver='liblinear',max_iter=100, tol=t, C=C)\n",
    "lr_clf.fit(X, classes)\n",
    "\n",
    "#affichage des précisions\n",
    "\n",
    "pred_lrt = lr_clf.predict(X)\n",
    "pred_lr = lr_clf.predict(X_test)\n",
    "\n",
    "# Métriques pour Régression Logistique\n",
    "print(\"Logistic Regression\")\n",
    "print(f\"Logistic Regression accuracy train={accuracy_score(classes, pred_lrt):.4f}, test={accuracy_score(true, pred_lr):.4f}\")\n",
    "print(f\"Logistic Regression recall train={recall_score(classes, pred_lrt, average='macro'):.4f}, test={recall_score(true, pred_lr, average='macro'):.4f}\")\n",
    "print(f\"Logistic Regression F1-score train={f1_score(classes, pred_lrt, average='macro'):.4f}, test={f1_score(true, pred_lr, average='macro'):.4f}\")\n",
    "print(\"--------------------------------------------------------------\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DRJ6owrUAzJ"
   },
   "source": [
    "performance should be worst than with bag of word (~80%). Sum/Mean aggregation does not work well on long reviews (especially with many frequent words). This adds a lot of noise.\n",
    "\n",
    "## **Todo**:  Try answering the following questions:\n",
    "\n",
    "- Which word2vec model works best: skip-gram or cbow\n",
    "- Do pretrained vectors work best than those learnt on the train dataset ?\n",
    "\n",
    "## **Todo**: evaluate the same pipeline on speaker ID task (Chirac/Mitterrand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bh98ivcQUAzJ"
   },
   "source": [
    "\n",
    "**(Bonus)** To have a better accuracy, we could try two things:\n",
    "- Better aggregation methods (weight by tf-idf ?)\n",
    "- Another word vectorizing method such as [fasttext](https://radimrehurek.com/gensim/models/fasttext.html)\n",
    "- A document vectorizing method such as [Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "902a52bcf4503a473db011f1937bdfe17613b08622219712e0110e48c4958c23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
